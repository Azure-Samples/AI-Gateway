{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4e23e5",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è Microsoft Foundry\n",
    "\n",
    "## Foundry Models Evals lab\n",
    "![flow](../../images/foundry-models-evals.gif)\n",
    "\n",
    "Playground to experiment with [Microsoft Foundry cloud evaluations](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation). This lab demonstrates how to extract LLM request/response data from Azure API Management's built-in logging (`ApiManagementGatewayLlmLog`) and use it as input for running evaluations in Microsoft Foundry.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "‚ñ∂Ô∏è Click `Run All` to execute all steps sequentially, or execute them `Step by Step`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f40fb",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management) \n",
    "- Adjust the models and versions according the [availability by region.](https://learn.microsoft.com/azure/ai-services/openai/concepts/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "sys.path.insert(1, '../../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"swedencentral\"  # Must be a region that supports risk and safety evaluators\n",
    "\n",
    "aiservices_config = [{\"name\": \"foundry1\", \"location\": \"swedencentral\"}]\n",
    "\n",
    "models_config = [{\"name\": \"gpt-4.1-mini\", \"publisher\": \"OpenAI\", \"version\": \"2025-04-14\", \"sku\": \"GlobalStandard\", \"capacity\": 100}]\n",
    "\n",
    "apim_sku = 'Basicv2'\n",
    "apim_subscriptions_config = [{\"name\": \"subscription1\", \"displayName\": \"Subscription 1\"}]\n",
    "\n",
    "inference_api_path = \"inference\"  # path to the inference API in the APIM service\n",
    "inference_api_type = \"AzureOpenAIV1\"  # options: AzureOpenAI, AzureAI, OpenAI, PassThrough\n",
    "inference_api_version = \"v1\"\n",
    "foundry_project_name = deployment_name\n",
    "\n",
    "utils.print_ok('Notebook initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b5728",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Verify the Azure CLI and the connected Azure subscription\n",
    "\n",
    "The following commands ensure that you have the latest version of the Azure CLI and that the Azure CLI is connected to your Azure subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = utils.run(\"az account show\", \"Retrieved az account\", \"Failed to get the current az account\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    current_user = output.json_data['user']['name']\n",
    "    tenant_id = output.json_data['tenantId']\n",
    "    subscription_id = output.json_data['id']\n",
    "\n",
    "    utils.print_info(f\"Current user: {current_user}\")\n",
    "    utils.print_info(f\"Tenant ID: {tenant_id}\")\n",
    "    utils.print_info(f\"Subscription ID: {subscription_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62a0ba",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This lab uses [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep) to declarative define all the resources that will be deployed in the specified resource group. Change the parameters or the [main.bicep](main.bicep) directly to try different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resource group if doesn't exist\n",
    "utils.create_resource_group(resource_group_name, resource_group_location)\n",
    "\n",
    "# Define the Bicep parameters\n",
    "bicep_parameters = {\n",
    "    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "    \"contentVersion\": \"1.0.0.0\",\n",
    "    \"parameters\": {\n",
    "        \"apimSku\": { \"value\": apim_sku },\n",
    "        \"aiServicesConfig\": { \"value\": aiservices_config },\n",
    "        \"modelsConfig\": { \"value\": models_config },\n",
    "        \"apimSubscriptionsConfig\": { \"value\": apim_subscriptions_config },\n",
    "        \"inferenceAPIPath\": { \"value\": inference_api_path },\n",
    "        \"inferenceAPIType\": { \"value\": inference_api_type },\n",
    "        \"foundryProjectName\": { \"value\": foundry_project_name },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write the parameters to the params.json file\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "# Run the deployment\n",
    "output = utils.run(f\"az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file main.bicep --parameters params.json\",\n",
    "    f\"Deployment '{deployment_name}' succeeded\", f\"Deployment '{deployment_name}' failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff43c07",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "We are now at the stage where we only need to retrieve the gateway URL and the subscription before we are ready for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5217659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all of the outputs from the deployment\n",
    "output = utils.run(f\"az deployment group show --name {deployment_name} -g {resource_group_name}\", f\"Retrieved deployment: {deployment_name}\", f\"Failed to retrieve deployment: {deployment_name}\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    log_analytics_id = utils.get_deployment_output(output, 'logAnalyticsWorkspaceId', 'Log Analytics Id')\n",
    "    apim_service_id = utils.get_deployment_output(output, 'apimServiceId', 'APIM Service Id')\n",
    "    apim_resource_gateway_url = utils.get_deployment_output(output, 'apimResourceGatewayURL', 'APIM API Gateway URL')\n",
    "    foundry_project_endpoint = utils.get_deployment_output(output, 'foundryProjectEndpoint', 'Foundry Project Endpoint')\n",
    "    apim_subscriptions = json.loads(utils.get_deployment_output(output, 'apimSubscriptions').replace(\"\\'\", \"\\\"\"))\n",
    "    for subscription in apim_subscriptions:\n",
    "        subscription_name = subscription['name']\n",
    "        subscription_key = subscription['key']\n",
    "        utils.print_info(f\"Subscription Name: {subscription_name}\")\n",
    "        utils.print_info(f\"Subscription Key: ****{subscription_key[-4:]}\")\n",
    "    api_key = apim_subscriptions[0].get(\"key\") # default api key to the first subscription key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47177b",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Generate test data by calling the API\n",
    "\n",
    "We'll make several API calls to generate LLM request/response data that will be logged to `ApiManagementGatewayLlmLog`. This data will later be extracted and used for evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d11979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests, time\n",
    "\n",
    "# Sample prompts to generate diverse evaluation data\n",
    "test_prompts = [\n",
    "    {\"system\": \"You are a helpful assistant.\", \"user\": \"What is the capital of France?\"},\n",
    "    {\"system\": \"You are a helpful assistant.\", \"user\": \"Explain quantum computing in simple terms.\"},\n",
    "    {\"system\": \"You are a helpful assistant.\", \"user\": \"Write a haiku about programming.\"},\n",
    "    {\"system\": \"You are a customer service agent.\", \"user\": \"I want to return a product I bought last week.\"},\n",
    "    {\"system\": \"You are a helpful assistant.\", \"user\": \"What are the benefits of exercise?\"},\n",
    "    {\"system\": \"You are a technical support agent.\", \"user\": \"My computer won't turn on. What should I do?\"},\n",
    "    {\"system\": \"You are a travel advisor.\", \"user\": \"Recommend a vacation destination for families.\"},\n",
    "    {\"system\": \"You are a helpful assistant.\", \"user\": \"Summarize the plot of Romeo and Juliet.\"},\n",
    "]\n",
    "\n",
    "url = f\"{apim_resource_gateway_url}/{inference_api_path}/openai/v1/chat/completions?api-version={inference_api_version}\"\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'api-key': api_key,\n",
    "    'x-user-id': 'eval-test-user'\n",
    "})\n",
    "\n",
    "try:\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"‚ñ∂Ô∏è Request {i+1}/{len(test_prompts)}: {prompt['user'][:50]}...\")\n",
    "        \n",
    "        messages = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": prompt['system']},\n",
    "                {\"role\": \"user\", \"content\": prompt['user']}\n",
    "            ], \n",
    "            \"model\": models_config[0]['name']\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = session.post(url, json=messages)\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "            print(f\"‚úÖ Response: {data.get('choices')[0].get('message').get('content')[:100]}...\")\n",
    "            print(f\"‚åö {response_time:.2f} seconds\\n\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.text}\\n\")\n",
    "        \n",
    "        time.sleep(0.5)  # Small delay between requests\n",
    "finally:\n",
    "    session.close()\n",
    "\n",
    "utils.print_ok(f\"Generated {len(test_prompts)} test requests for evaluation\")\n",
    "utils.print_info(\"Wait a few minutes for logs to be ingested into Log Analytics before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02830c35",
   "metadata": {},
   "source": [
    "<a id='extract'></a>\n",
    "### üìä Extract LLM logs from API Management\n",
    "\n",
    "Query the `ApiManagementGatewayLlmLog` table to extract prompts and completions. This data will be formatted for use with Microsoft Foundry evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Query to extract LLM request/response data from APIM logs\n",
    "query = \"ApiManagementGatewayLlmLog \\\n",
    "| where TimeGenerated > ago(1h) \\\n",
    "| project TimeGenerated, CorrelationId, DeploymentName, ModelName, TotalTokens, RequestMessages, ResponseMessages \\\n",
    "| summarize TimeGenerated = max(TimeGenerated), DeploymentName = take_any(DeploymentName), ModelName = take_any(ModelName), TotalTokens = sum(TotalTokens), RequestMessages = take_any(RequestMessages), ResponseMessages = take_any(ResponseMessages) by CorrelationId \\\n",
    "| take 50\"\n",
    "\n",
    "output = utils.run(f'az monitor log-analytics query -w {log_analytics_id} --analytics-query \"{query}\"', \n",
    "                   \"Retrieved LLM logs\", \"Failed to retrieve LLM logs\")\n",
    "\n",
    "llm_logs = []\n",
    "if output.success and output.json_data:\n",
    "    llm_logs = output.json_data\n",
    "    df = pd.DataFrame(llm_logs)\n",
    "    utils.print_ok(f\"Retrieved {len(llm_logs)} log entries\")\n",
    "    display(df[['TimeGenerated', 'DeploymentName', 'ModelName', 'TotalTokens', 'RequestMessages', 'ResponseMessages']].head(10))\n",
    "else:\n",
    "    utils.print_warning(\"No logs found. Wait a few minutes for logs to be ingested and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af4602",
   "metadata": {},
   "source": [
    "<a id='transform'></a>\n",
    "### üîÑ Transform logs to evaluation dataset format\n",
    "\n",
    "Convert the extracted LLM logs into the JSONL format required by Microsoft Foundry evaluations. The format includes `query` (user input) and `response` (model output) fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a070965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_request_messages(request_messages_str):\n",
    "    \"\"\"Extract the user query from request messages.\"\"\"\n",
    "    try:\n",
    "        if isinstance(request_messages_str, str):\n",
    "            messages = json.loads(request_messages_str)\n",
    "        else:\n",
    "            messages = request_messages_str\n",
    "        \n",
    "        # Find the user message\n",
    "        for msg in messages:\n",
    "            if msg.get('role') == 'user':\n",
    "                return msg.get('content', '')\n",
    "        return ''\n",
    "    except:\n",
    "        return str(request_messages_str)\n",
    "\n",
    "def parse_response_content(response_messages_str):\n",
    "    \"\"\"Extract the assistant response from response content.\"\"\"\n",
    "    try:\n",
    "        if isinstance(response_messages_str, str):\n",
    "            content = json.loads(response_messages_str)\n",
    "        else:\n",
    "            content = response_messages_str\n",
    "        \n",
    "        # Handle different response formats\n",
    "        if isinstance(content, dict):\n",
    "            if 'choices' in content:\n",
    "                return content['choices'][0].get('message', {}).get('content', '')\n",
    "            elif 'content' in content:\n",
    "                return content['content']\n",
    "        return str(content)\n",
    "    except:\n",
    "        return str(response_messages_str)\n",
    "\n",
    "# Transform logs to evaluation format\n",
    "eval_data = []\n",
    "for log in llm_logs:\n",
    "    query = parse_request_messages(log.get('RequestMessages', ''))\n",
    "    response = parse_response_content(log.get('ResponseMessages', ''))\n",
    "    \n",
    "    if query and response:\n",
    "        eval_data.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"context\": f\"Model: {log.get('ModelName', 'unknown')}\",\n",
    "            \"ground_truth\": \"\"  # Can be populated manually for specific evaluations\n",
    "        })\n",
    "\n",
    "utils.print_ok(f\"Transformed {len(eval_data)} entries for evaluation\")\n",
    "\n",
    "# Preview the evaluation data\n",
    "if eval_data:\n",
    "    print(\"\\nüìã Sample evaluation entry:\")\n",
    "    print(json.dumps(eval_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c54c5",
   "metadata": {},
   "source": [
    "<a id='save'></a>\n",
    "### üíæ Save evaluation dataset to JSONL file\n",
    "\n",
    "Save the transformed data as a JSONL file that can be uploaded to Microsoft Foundry for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147379d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "eval_data_file = \"evaluation_data.jsonl\"\n",
    "\n",
    "with open(eval_data_file, 'w') as f:\n",
    "    for entry in eval_data:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "utils.print_ok(f\"Saved {len(eval_data)} entries to {eval_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8326f5",
   "metadata": {},
   "source": [
    "<a id='foundry'></a>\n",
    "### üöÄ Run evaluation in Microsoft Foundry\n",
    "\n",
    "Upload the evaluation dataset to Microsoft Foundry and run evaluations using built-in evaluators like coherence, fluency, and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312be75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Create the Foundry project client\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=foundry_project_endpoint,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "utils.print_ok(\"Connected to Microsoft Foundry project\")\n",
    "\n",
    "# Upload the evaluation dataset\n",
    "dataset_name = f\"apim-llm-logs-{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}\"\n",
    "dataset_version = \"1\"\n",
    "\n",
    "dataset = project_client.datasets.upload_file(\n",
    "    name=dataset_name,\n",
    "    version=dataset_version,\n",
    "    file_path=eval_data_file,\n",
    ")\n",
    "\n",
    "utils.print_ok(f\"Uploaded dataset: {dataset_name}\")\n",
    "utils.print_info(f\"Dataset ID: {dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678614d5",
   "metadata": {},
   "source": [
    "<a id='create-eval'></a>\n",
    "### üìù Create and run the evaluation\n",
    "\n",
    "Define the evaluation criteria and run the evaluation using Microsoft Foundry's built-in evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64794c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from pprint import pprint\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from openai.types.evals.create_eval_jsonl_run_data_source_param import (\n",
    "    CreateEvalJSONLRunDataSourceParam,\n",
    "    SourceFileID,\n",
    ")\n",
    "\n",
    "model_deployment_name = models_config[0]['name']\n",
    "\n",
    "with DefaultAzureCredential() as credential:\n",
    "    with AIProjectClient(endpoint=foundry_project_endpoint, credential=credential) as project_client:\n",
    "        \n",
    "        print(\"Creating an OpenAI client from the AI Project client\")\n",
    "        client = project_client.get_openai_client(api_version=\"2025-04-01-preview\")\n",
    "\n",
    "        # Upload file using OpenAI Files API (NOT Foundry datasets)\n",
    "        print(\"Uploading file using OpenAI Files API...\")\n",
    "        with open(eval_data_file, \"rb\") as f:\n",
    "            uploaded_file = client.files.create(\n",
    "                file=f,\n",
    "                purpose=\"evals\"\n",
    "            )\n",
    "        print(f\"Uploaded file ID: {uploaded_file.id}\")\n",
    "\n",
    "        data_source_config = {\n",
    "            \"type\": \"custom\",\n",
    "            \"item_schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\"},\n",
    "                    \"response\": {\"type\": \"string\"},\n",
    "                    \"context\": {\"type\": \"string\"},\n",
    "                    \"ground_truth\": {\"type\": \"string\"},\n",
    "                },\n",
    "                \"required\": [],\n",
    "            },\n",
    "            \"include_sample_schema\": False,\n",
    "        }\n",
    "\n",
    "        testing_criteria = [\n",
    "            {\n",
    "                \"type\": \"label_model\",\n",
    "                \"name\": \"quality_check\",\n",
    "                \"model\": model_deployment_name,\n",
    "                \"input\": [\n",
    "                    {\"role\": \"system\", \"content\": \"Rate the response quality as 'good' or 'bad'.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"Query: {{item.query}}\\nResponse: {{item.response}}\"}\n",
    "                ],\n",
    "                \"labels\": [\"good\", \"bad\"],\n",
    "                \"passing_labels\": [\"good\"]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        print(\"Creating Eval Group\")\n",
    "        eval_object = client.evals.create(\n",
    "            name=f\"APIM LLM Logs Evaluation - {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M')}\",\n",
    "            data_source_config=data_source_config,\n",
    "            testing_criteria=testing_criteria,\n",
    "        )\n",
    "        print(f\"Eval Group created: {eval_object.id}\")\n",
    "\n",
    "        print(\"Creating Eval Run with OpenAI File ID\")\n",
    "        eval_run_object = client.evals.runs.create(\n",
    "            eval_id=eval_object.id,\n",
    "            name=f\"apim-logs-eval-run-{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}\",\n",
    "            metadata={\"source\": \"ApiManagementGatewayLlmLog\", \"lab\": \"foundry-evals\"},\n",
    "            data_source=CreateEvalJSONLRunDataSourceParam(\n",
    "                type=\"jsonl\",\n",
    "                source=SourceFileID(\n",
    "                    type=\"file_id\",\n",
    "                    id=uploaded_file.id,  # Use OpenAI file ID, not dataset.id\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        print(f\"Eval Run created: {eval_run_object.id}\")\n",
    "\n",
    "        # Poll until the run completes or fails\n",
    "        while True:\n",
    "            run = client.evals.runs.retrieve(\n",
    "                run_id=eval_run_object.id, \n",
    "                eval_id=eval_object.id\n",
    "            )\n",
    "            print(f\"Status: {run.status}\")\n",
    "            \n",
    "            if run.status in (\"completed\", \"failed\"):\n",
    "                output_items = list(\n",
    "                    client.evals.runs.output_items.list(\n",
    "                        run_id=run.id, \n",
    "                        eval_id=eval_object.id\n",
    "                    )\n",
    "                )\n",
    "                pprint(output_items[:3])  # Show first 3 results\n",
    "                print(f\"\\nüîó Eval Run Report URL: {run.report_url}\")\n",
    "                break\n",
    "\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e743527",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
