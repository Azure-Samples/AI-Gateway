{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02699cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# a2a_agents.py\n",
    "# ------------------------------------------------------------------------\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import abc\n",
    "from collections.abc import AsyncIterable\n",
    "from typing import TYPE_CHECKING, Annotated, Any, Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    OpenAIChatPromptExecutionSettings,\n",
    "    AzureChatCompletion\n",
    ")\n",
    "from semantic_kernel.contents import (\n",
    "    FunctionCallContent,\n",
    "    FunctionResultContent,\n",
    "    StreamingChatMessageContent,\n",
    "    StreamingTextContent,\n",
    ")\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "\n",
    "import asyncio\n",
    "from semantic_kernel.connectors.mcp import MCPSsePlugin\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from semantic_kernel.contents import ChatMessageContent\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# region Response Format\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"A Response Format model to direct how the model should respond.\"\"\"\n",
    "\n",
    "    status: Literal['input_required', 'completed', 'error'] = 'input_required'\n",
    "    message: str\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "class AbstractAgent(abc.ABC):\n",
    "    \"\"\"\n",
    "    A minimal, implementation-agnostic contract for any assistant agent using any framework.\n",
    "\n",
    "    Concrete subclasses may wrap Semantic Kernel, LangChain, your own\n",
    "    in-house stack, or even a local LLM – as long as they satisfy this API.\n",
    "    \"\"\"\n",
    "\n",
    "    #: MIME types that downstream code can rely on receiving.\n",
    "    SUPPORTED_CONTENT_TYPES: list[str] = ['text', 'text/plain']\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Lifecycle helpers\n",
    "    # ------------------------------------------------------------------ #\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        # subclasses may override; by default do nothing\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        # subclasses may override; by default do nothing\n",
    "        return False                     # propagate any exception\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    async def invoke(self, user_input: str, session_id: str) -> dict[str, Any]:  # noqa: D401\n",
    "        \"\"\"\n",
    "        Handle a *single-shot* request.\n",
    "\n",
    "        Implementations **must** be idempotent: calling twice with the same\n",
    "        `(user_input, session_id)` pair should yield the same logical answer,\n",
    "        even if the underlying LLM re-generates new text.\n",
    "        \"\"\"\n",
    "\n",
    "# region Semantic Kernel Agent\n",
    "\n",
    "\n",
    "class SemanticKernelAgent(AbstractAgent):\n",
    "    \"\"\"Wraps Semantic Kernel-based agents to handle tasks.\"\"\"\n",
    "\n",
    "    # agent: ChatCompletionAgent\n",
    "    # thread: ChatHistoryAgentThread = None\n",
    "    # mcp_plugin: MCPSsePlugin = None\n",
    "    # mcp_url: str = None\n",
    "\n",
    "\n",
    "    SUPPORTED_CONTENT_TYPES = ['text', 'text/plain']\n",
    "\n",
    "    def __init__(self, mcp_url: str, title: str,\n",
    "                 oai_client: ChatCompletionClientBase):\n",
    "        # just stash config – DO NOT build anything heavy here\n",
    "        self._mcp_url   = mcp_url.rstrip('/')\n",
    "        self._title     = title\n",
    "        self._oai_client = oai_client\n",
    "\n",
    "        # runtime attributes populated in __aenter__\n",
    "        self.mcp_plugin: MCPSsePlugin | None = None\n",
    "        self.agent:      ChatCompletionAgent | None = None\n",
    "        self.thread:     ChatHistoryAgentThread | None = None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # async context-manager wires the plugin correctly\n",
    "    async def __aenter__(self) -> \"SemanticKernelAgent\":\n",
    "        # 1. open the SSE plugin\n",
    "        self.mcp_plugin = MCPSsePlugin(\n",
    "            name        = self._title,\n",
    "            url         = self._mcp_url,\n",
    "            description = f\"{self._title} Plugin\",\n",
    "        )\n",
    "        await self.mcp_plugin.__aenter__()            # <-- crucial\n",
    "\n",
    "        # 2. build the SK agent (note the **singular** `plugin=`)\n",
    "        self.agent = ChatCompletionAgent(\n",
    "            service = self._oai_client,\n",
    "            name    = f\"{self._title}_agent\",\n",
    "            instructions = (\n",
    "                f\"You are a helpful assistant for {self._title} queries.\"\n",
    "            ),\n",
    "            plugins  = [self.mcp_plugin],    \n",
    "            arguments = KernelArguments(\n",
    "                settings = OpenAIChatPromptExecutionSettings(\n",
    "                    response_format = ResponseFormat,\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        if self.thread:\n",
    "            await self.thread.delete()\n",
    "            self.thread = None\n",
    "        if self.mcp_plugin:\n",
    "            await self.mcp_plugin.__aexit__(exc_type, exc, tb)\n",
    "        return False\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    async def invoke(self, user_input: str, session_id: str) -> dict[str, Any]:\n",
    "        \"\"\"Handle synchronous tasks (like tasks/send).\n",
    "\n",
    "        Args:\n",
    "            user_input (str): User input message.\n",
    "            session_id (str): Unique identifier for the session.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the content, task completion status, and user input requirement.\n",
    "        \"\"\"\n",
    "        await self._ensure_thread_exists(session_id)\n",
    "\n",
    "        # Use SK's get_response for a single shot\n",
    "        response = await self.agent.get_response(\n",
    "            messages=user_input,\n",
    "            thread=self.thread,\n",
    "        )\n",
    "        return self._get_agent_response(response.content)\n",
    "\n",
    "    async def stream(\n",
    "        self,\n",
    "        user_input: str,\n",
    "        session_id: str,\n",
    "    ) -> AsyncIterable[dict[str, Any]]:\n",
    "        \"\"\"For streaming tasks we yield the SK agent's invoke_stream progress.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): User input message.\n",
    "            session_id (str): Unique identifier for the session.\n",
    "\n",
    "        Yields:\n",
    "            dict: A dictionary containing the content, task completion status,\n",
    "            and user input requirement.\n",
    "        \"\"\"\n",
    "        await self._ensure_thread_exists(session_id)\n",
    "\n",
    "        plugin_notice_seen = False\n",
    "        plugin_event = asyncio.Event()\n",
    "\n",
    "        text_notice_seen = False\n",
    "        chunks: list[StreamingChatMessageContent] = []\n",
    "\n",
    "        async def _handle_intermediate_message(\n",
    "            message: 'ChatMessageContent',\n",
    "        ) -> None:\n",
    "            \"\"\"Handle intermediate messages from the agent.\"\"\"\n",
    "            nonlocal plugin_notice_seen\n",
    "            if not plugin_notice_seen:\n",
    "                plugin_notice_seen = True\n",
    "                plugin_event.set()\n",
    "            # An example of handling intermediate messages during function calling\n",
    "            for item in message.items or []:\n",
    "                if isinstance(item, FunctionResultContent):\n",
    "                    print(\n",
    "                        f'############ Function Result:> {item.result} for function: {item.name}'\n",
    "                    )\n",
    "                elif isinstance(item, FunctionCallContent):\n",
    "                    print(\n",
    "                        f'############ Function Call:> {item.name} with arguments: {item.arguments}'\n",
    "                    )\n",
    "                else:\n",
    "                    print(f'############ Message:> {item}')\n",
    "\n",
    "        async for chunk in self.agent.invoke_stream(\n",
    "            messages=user_input,\n",
    "            thread=self.thread,\n",
    "            on_intermediate_message=_handle_intermediate_message,\n",
    "        ):\n",
    "            if plugin_event.is_set():\n",
    "                yield {\n",
    "                    'is_task_complete': False,\n",
    "                    'require_user_input': False,\n",
    "                    'content': 'Processing function calls...',\n",
    "                }\n",
    "                plugin_event.clear()\n",
    "\n",
    "            if any(isinstance(i, StreamingTextContent) for i in chunk.items):\n",
    "                if not text_notice_seen:\n",
    "                    yield {\n",
    "                        'is_task_complete': False,\n",
    "                        'require_user_input': False,\n",
    "                        'content': 'Building the output...',\n",
    "                    }\n",
    "                    text_notice_seen = True\n",
    "                chunks.append(chunk.message)\n",
    "\n",
    "        if chunks:\n",
    "            yield self._get_agent_response(sum(chunks[1:], chunks[0]))\n",
    "\n",
    "    def _get_agent_response(\n",
    "        self, message: 'ChatMessageContent'\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Extracts the structured response from the agent's message content.\n",
    "\n",
    "        Args:\n",
    "            message (ChatMessageContent): The message content from the agent.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the content, task completion status, and user input requirement.\n",
    "        \"\"\"\n",
    "        structured_response = ResponseFormat.model_validate_json(\n",
    "            message.content\n",
    "        )\n",
    "\n",
    "        default_response = {\n",
    "            'is_task_complete': False,\n",
    "            'require_user_input': True,\n",
    "            'content': 'We are unable to process your request at the moment. Please try again.',\n",
    "        }\n",
    "\n",
    "        if isinstance(structured_response, ResponseFormat):\n",
    "            response_map = {\n",
    "                'input_required': {\n",
    "                    'is_task_complete': False,\n",
    "                    'require_user_input': True,\n",
    "                },\n",
    "                'error': {\n",
    "                    'is_task_complete': False,\n",
    "                    'require_user_input': True,\n",
    "                },\n",
    "                'completed': {\n",
    "                    'is_task_complete': True,\n",
    "                    'require_user_input': False,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            response = response_map.get(structured_response.status)\n",
    "            if response:\n",
    "                return {**response, 'content': structured_response.message}\n",
    "\n",
    "        return default_response\n",
    "\n",
    "    async def _ensure_thread_exists(self, session_id: str) -> None:\n",
    "        \"\"\"Ensure the thread exists for the given session ID.\n",
    "\n",
    "        Args:\n",
    "            session_id (str): Unique identifier for the session.\n",
    "        \"\"\"\n",
    "        if self.thread is None or self.thread.id != session_id:\n",
    "            await self.thread.delete() if self.thread else None\n",
    "            self.thread = ChatHistoryAgentThread(thread_id=session_id)\n",
    "\n",
    "\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# a2a_agent_exec.py\n",
    "# ------------------------------------------------------------------------\n",
    "import logging\n",
    "\n",
    "from a2a.server.agent_execution import AgentExecutor, RequestContext\n",
    "from a2a.server.events.event_queue import EventQueue\n",
    "from a2a.types import (\n",
    "    TaskArtifactUpdateEvent,\n",
    "    TaskState,\n",
    "    TaskStatus,\n",
    "    TaskStatusUpdateEvent,\n",
    ")\n",
    "from a2a.utils import (\n",
    "    new_agent_text_message,\n",
    "    new_data_artifact,\n",
    "    new_task,\n",
    "    new_text_artifact,\n",
    ")\n",
    "\n",
    "# from a2a_agents import AbstractAgent\n",
    "from typing_extensions import override\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class A2ALabAgentExecutor(AgentExecutor):\n",
    "    \"\"\" Agent Executor \"\"\"\n",
    "\n",
    "    def __init__(self, agent: AbstractAgent):\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "\n",
    "    # ---------------- async context-manager glue -------------\n",
    "    async def __aenter__(self):\n",
    "        if hasattr(self.agent, \"__aenter__\"):\n",
    "            await self.agent.__aenter__()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        if hasattr(self.agent, \"__aexit__\"):\n",
    "            await self.agent.__aexit__(exc_type, exc, tb)\n",
    "        return False\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    async def execute(\n",
    "        self,\n",
    "        context: RequestContext,\n",
    "        event_queue: EventQueue,\n",
    "    ) -> None:\n",
    "        query = context.get_user_input()\n",
    "        task = context.current_task\n",
    "        if not task:\n",
    "            task = new_task(context.message)\n",
    "            event_queue.enqueue_event(task)\n",
    "\n",
    "        async for partial in self.agent.stream(query, task.contextId):\n",
    "            require_input = partial['require_user_input']\n",
    "            is_done = partial['is_task_complete']\n",
    "            text_content = partial['content']\n",
    "\n",
    "            if require_input:\n",
    "                event_queue.enqueue_event(\n",
    "                    TaskStatusUpdateEvent(\n",
    "                        status=TaskStatus(\n",
    "                            state=TaskState.input_required,\n",
    "                            message=new_agent_text_message(\n",
    "                                text_content,\n",
    "                                task.contextId,\n",
    "                                task.id,\n",
    "                            ),\n",
    "                        ),\n",
    "                        final=True,\n",
    "                        contextId=task.contextId,\n",
    "                        taskId=task.id,\n",
    "                    )\n",
    "                )\n",
    "            elif is_done:\n",
    "                event_queue.enqueue_event(\n",
    "                    TaskArtifactUpdateEvent(\n",
    "                        append=False,\n",
    "                        contextId=task.contextId,\n",
    "                        taskId=task.id,\n",
    "                        lastChunk=True,\n",
    "                        artifact=new_text_artifact(\n",
    "                            name='current_result',\n",
    "                            description='Result of request to agent.',\n",
    "                            text=text_content,\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "                event_queue.enqueue_event(\n",
    "                    TaskStatusUpdateEvent(\n",
    "                        status=TaskStatus(state=TaskState.completed),\n",
    "                        final=True,\n",
    "                        contextId=task.contextId,\n",
    "                        taskId=task.id,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                event_queue.enqueue_event(\n",
    "                    TaskStatusUpdateEvent(\n",
    "                        status=TaskStatus(\n",
    "                            state=TaskState.working,\n",
    "                            message=new_agent_text_message(\n",
    "                                text_content,\n",
    "                                task.contextId,\n",
    "                                task.id,\n",
    "                            ),\n",
    "                        ),\n",
    "                        final=False,\n",
    "                        contextId=task.contextId,\n",
    "                        taskId=task.id,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    async def cancel(\n",
    "        self, context: RequestContext, event_queue: EventQueue\n",
    "    ) -> None:\n",
    "        raise Exception('cancel not supported')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import httpx\n",
    "\n",
    "from starlette.applications import Starlette     # A2A wraps Starlette\n",
    "from a2a.server.apps import A2AStarletteApplication\n",
    "from a2a.server.request_handlers import DefaultRequestHandler\n",
    "from a2a.server.tasks import InMemoryTaskStore, InMemoryPushNotifier\n",
    "from a2a.types import AgentCapabilities, AgentCard, AgentSkill\n",
    "\n",
    "# from a2a_agents import AbstractAgent            # your own base class\n",
    "# from a2a_agent_exec import A2ALabAgentExecutor\n",
    "# from a2a_agents import SemanticKernelAgent\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "TITLE                   = \"Weather\"\n",
    "MCP_URL                 = \"/weather/sse\"\n",
    "APIM_GATEWAY_URL        = \"https://apim-ulp3pavelxqec.azure-api.net\"\n",
    "APIM_SUBSCRIPTION_KEY   = \"**************\"  # Keep it secret! Keep it safe!\n",
    "OPENAI_API_VERSION      = \"2024-10-21\"\n",
    "OPENAI_DEPLOYMENT_NAME  = \"gpt-4o-mini\"\n",
    "ACA_URL                 = f\"https://{os.environ.get('CONTAINER_APP_NAME', '')}.{os.environ.get('CONTAINER_APP_ENV_DNS_SUFFIX', '')}\"\n",
    "A2A_URL                 = \"http://localhost:10020\"\n",
    "\n",
    "def build_app(\n",
    "    *,\n",
    "    host: str = \"localhost\",\n",
    "    port: int = 10020,\n",
    ") -> Starlette:\n",
    "    \"\"\"\n",
    "    Assemble and return the fully-wired Starlette ASGI application.\n",
    "\n",
    "    This function:\n",
    "      • creates the Semantic Kernel agent\n",
    "      • wraps it in an A2A executor\n",
    "      • registers startup/shutdown hooks so the SSE socket is opened/closed\n",
    "      • builds the A2A Starlette application and returns it\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- 1. Create the naked SemanticKernelAgent -----------------\n",
    "    weather_agent = SemanticKernelAgent(\n",
    "        mcp_url=f\"{APIM_GATEWAY_URL}{MCP_URL}\",\n",
    "        title=TITLE,\n",
    "        oai_client=AzureChatCompletion(\n",
    "            endpoint=APIM_GATEWAY_URL,\n",
    "            api_key=APIM_SUBSCRIPTION_KEY,\n",
    "            api_version=OPENAI_API_VERSION,\n",
    "            deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # -------- 2. Wrap it in the A2A executor --------------------------\n",
    "    sk_weather_agent_exec = A2ALabAgentExecutor(agent=weather_agent)\n",
    "\n",
    "    # -------- 3. Wire the executor into the default request handler ---\n",
    "    httpx_client   = httpx.AsyncClient()\n",
    "    request_handler = DefaultRequestHandler(\n",
    "        agent_executor = sk_weather_agent_exec,\n",
    "        task_store     = InMemoryTaskStore(),\n",
    "        push_notifier  = InMemoryPushNotifier(httpx_client),\n",
    "    )\n",
    "\n",
    "    # -------- 4. Build the A2A server via Starlette -------------------\n",
    "    server = A2AStarletteApplication(\n",
    "        agent_card   = _get_agent_card(host, port),\n",
    "        http_handler = request_handler,\n",
    "    )\n",
    "    app: Starlette = server.build()\n",
    "\n",
    "    # -------- 5. Register lifecycle hooks to open/close the agent -----\n",
    "    @app.on_event(\"startup\")\n",
    "    async def _startup() -> None:\n",
    "        log.info(\"Opening SemanticKernelAgent SSE connection …\")\n",
    "        await weather_agent.__aenter__()          # opens MCPSsePlugin\n",
    "        # NB: if you decide to make the *executor* the context\n",
    "        # manager (Option 2), just call `await sk_weather_agent_exec.__aenter__()`\n",
    "\n",
    "    @app.on_event(\"shutdown\")\n",
    "    async def _shutdown() -> None:\n",
    "        log.info(\"Closing SemanticKernelAgent SSE connection …\")\n",
    "        await weather_agent.__aexit__(None, None, None)\n",
    "        await httpx_client.aclose()\n",
    "\n",
    "    return app\n",
    "\n",
    "\n",
    "# ========== Helper: build the agent-card sent to A2A clients ===========\n",
    "def _get_agent_card(host: str, port: int) -> AgentCard:\n",
    "    capabilities = AgentCapabilities(streaming=True)\n",
    "\n",
    "    skill_weather = AgentSkill(\n",
    "        id='weather_forecast_sk',\n",
    "        name='Semantic Kernel Weather forecasting agent',\n",
    "        description='Answers questions about the weather anywhere in the world',\n",
    "        tags=['weather', 'semantic-kernel'],\n",
    "        examples=[\n",
    "            \"What's the weather like in Cairo?\",\n",
    "            \"Is it raining today in London and Paris?\",\n",
    "            \"What's the capital of Sweden?\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return AgentCard(\n",
    "        name='SK Weather Agent',\n",
    "        description='Semantic-Kernel-powered weather agent',\n",
    "        url=f'http://{host}:{port}/',\n",
    "        version='1.0.0',\n",
    "        defaultInputModes=['text'],\n",
    "        defaultOutputModes=['text'],\n",
    "        capabilities=capabilities,\n",
    "        skills=[skill_weather],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# run_server.py  – launch with `python run_server.py`\n",
    "# ------------------------------------------------------------------------\n",
    "import uvicorn, nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = build_app()                  # Starlette ASGI application\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=10020)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
