{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "sys.path.insert(1, '../../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"westeurope\"\n",
    "\n",
    "apim_sku = 'Basicv2'\n",
    "\n",
    "openai_resources = [ {\"name\": \"openai1\", \"location\": \"uksouth\"}]\n",
    "openai_model_name = \"gpt-4o-mini\"\n",
    "openai_model_version = \"2024-07-18\"\n",
    "openai_model_sku = \"GlobalStandard\"\n",
    "openai_deployment_name = \"gpt-4o-mini\"\n",
    "openai_api_version = \"2024-10-21\"\n",
    "\n",
    "build = 0\n",
    "weather_mcp_server_image = \"weather-mcp-server\"\n",
    "weather_mcp_server_src = \"src/weather/mcp-server\"\n",
    "\n",
    "oncall_mcp_server_image = \"oncall-mcp-server\"\n",
    "oncall_mcp_server_src = \"src/oncall/mcp-server\"\n",
    "\n",
    "github_mcp_server_image = \"github-mcp-server\"\n",
    "github_mcp_server_src = \"src/github/mcp-server\"\n",
    "\n",
    "servicenow_mcp_server_image = \"servicenow-mcp-server\"\n",
    "servicenow_mcp_server_src = \"src/servicenow/mcp-server\"\n",
    "servicenow_instance_name = \"\" # Add here the name of your ServiceNow instance, e.g. \"businessname-dev\". Leave empty if you don't want to use ServiceNow.\n",
    "\n",
    "utils.print_ok('Notebook initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all of the outputs from the deployment\n",
    "output = utils.run(f\"az deployment group show --name {deployment_name} -g {resource_group_name}\", f\"Retrieved deployment: {deployment_name}\", f\"Failed to retrieve deployment: {deployment_name}\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    apim_service_id = utils.get_deployment_output(output, 'apimServiceId', 'APIM Service Id')\n",
    "    apim_resource_gateway_url = utils.get_deployment_output(output, 'apimResourceGatewayURL', 'APIM Gateway URL')\n",
    "    apim_resource_name = utils.get_deployment_output(output, 'apimResourceName', 'APIM Resource Name')\n",
    "    apim_subscription_key = utils.get_deployment_output(output, 'apimSubscriptionKey', 'APIM Subscription Key (masked)', True)\n",
    "    app_insights_name = utils.get_deployment_output(output, 'applicationInsightsName', 'Application Insights Name')\n",
    "    container_registry_name = utils.get_deployment_output(output, 'containerRegistryName', 'Container Registry Name')\n",
    "    weather_containerapp_resource_name = utils.get_deployment_output(output, 'weatherMCPServerContainerAppResourceName', 'Weather Container App Resource Name')\n",
    "    oncall_containerapp_resource_name = utils.get_deployment_output(output, 'oncallMCPServerContainerAppResourceName', 'Oncall Container App Resource Name')\n",
    "\n",
    "    a2a_weather_containerapp_resource_name = utils.get_deployment_output(output, 'a2AWeatherAgentServerContainerAppResourceName', 'A2A (Weather) Agent Container App Resource Name')\n",
    "    a2a_oncall_containerapp_resource_name = utils.get_deployment_output(output, 'a2AOncallAgentServerContainerAppResourceName', 'A2A (Oncall) Agent Container App Resource Name')\n",
    "\n",
    "    a2a_weather_a2a_agent_ep = utils.get_deployment_output(output, 'a2AWeatherAgentServerContainerAppFQDN', 'A2A (Weather) Agent Endpoint')\n",
    "    a2a_oncall_a2a_agent_ep = utils.get_deployment_output(output, 'a2AOncallAgentServerContainerAppFQDN', 'A2A (Oncall) Agent Endpoint')\n",
    "\n",
    "inference_api_path = \"\"\n",
    "inference_api_version = \"2025-03-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aca6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.mcp import MCPSsePlugin\n",
    "\n",
    "async def _safe_disconnect(plugin: MCPSsePlugin) -> None:\n",
    "    \"\"\"Handle SK method differences across versions.\"\"\"\n",
    "    for method_name in (\"disconnect\", \"close\", \"aclose\"):\n",
    "        method = getattr(plugin, method_name, None)\n",
    "        if method:\n",
    "            maybe_coro = method()\n",
    "            if asyncio.iscoroutine(maybe_coro):\n",
    "                await maybe_coro\n",
    "            return\n",
    "\n",
    "async def build_agent():\n",
    "    # Connect the agent to Azure OpenAI\n",
    "    service = AzureChatCompletion(\n",
    "            endpoint=f\"{apim_resource_gateway_url}/{inference_api_path}\",\n",
    "            api_key=apim_subscription_key,\n",
    "            api_version=inference_api_version,                \n",
    "            deployment_name=openai_model_name  # Use the first model from the models_config\n",
    "        )\n",
    "\n",
    "    # Attach a remote MCP plugin the agent can call during reasoning\n",
    "    # (e.g., a weather or tools server you already host elsewhere)\n",
    "    weather_plugin = MCPSsePlugin(\n",
    "        name=\"Weather\",\n",
    "        url=f\"{apim_resource_gateway_url}/weather/sse\",\n",
    "        description=\"Remote Weather MCP Plugin via SSE\",\n",
    "    )\n",
    "\n",
    "    await weather_plugin.connect()\n",
    "\n",
    "    agent = ChatCompletionAgent(\n",
    "        service=service,\n",
    "        name=\"WeatherAgent\",\n",
    "        instructions=(\n",
    "            \"You are a helpful assistant. \"\n",
    "            \"Use the 'Weather' plugin when the user asks about weather or locations. \"\n",
    "            \"Cite the source if appropriate.\"\n",
    "        ),\n",
    "        plugins=[weather_plugin],\n",
    "    )\n",
    "\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1849402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "from typing import Any, Literal\n",
    "\n",
    "from starlette.responses import Response\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
    "\n",
    "async def run(transport: Literal[\"sse\", \"stdio\", \"http\"] = \"stdio\", port: int | None = None) -> None:\n",
    "    kernel = await build_agent()\n",
    "\n",
    "    @kernel_function()\n",
    "    def echo_function(message: str, extra: str = \"\") -> str:\n",
    "        \"\"\"Echo a message as a function\"\"\"\n",
    "        return f\"Function echo: {message} {extra}\"\n",
    "\n",
    "    server = kernel.as_mcp_server(server_name=\"sk\")\n",
    "\n",
    "    if transport == \"http\" and port is not None:\n",
    "        import contextlib\n",
    "        from mcp.server.streamable_http_manager import StreamableHTTPSessionManager\n",
    "        from mcp.server.fastmcp import FastMCP\n",
    "        from starlette.types import Receive, Scope, Send\n",
    "        from starlette.applications import Starlette\n",
    "        from typing import AsyncIterator\n",
    "        from starlette.routing import Mount\n",
    "        import uvicorn\n",
    "\n",
    "        session_manager = StreamableHTTPSessionManager(\n",
    "            app=server,\n",
    "            event_store=None,\n",
    "            json_response=True,\n",
    "            stateless=True,\n",
    "            )\n",
    "        async def handle_streamable_http(scope: Scope, receive: Receive, send: Send) -> None:\n",
    "            await session_manager.handle_request(scope, receive, send)\n",
    "\n",
    "        @contextlib.asynccontextmanager\n",
    "        async def lifespan(app: Starlette) -> AsyncIterator[None]:\n",
    "            \"\"\"Context manager for session manager.\"\"\"\n",
    "            async with session_manager.run():\n",
    "                try:\n",
    "                    yield\n",
    "                finally:\n",
    "                    print(\"Application shutting down...\")\n",
    "        \n",
    "        starlette_app = Starlette(\n",
    "            debug=True,\n",
    "                routes=[\n",
    "                    Mount(\"/mcp\", app=handle_streamable_http),\n",
    "                ],\n",
    "                lifespan=lifespan,\n",
    "            )\n",
    "        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "    if transport == \"sse\" and port is not None:\n",
    "        import uvicorn\n",
    "        from mcp.server.sse import SseServerTransport\n",
    "        from starlette.applications import Starlette\n",
    "        from starlette.routing import Mount, Route\n",
    "\n",
    "        sse = SseServerTransport(\"/messages/\")\n",
    "\n",
    "        async def handle_sse(request):\n",
    "            async with sse.connect_sse(request.scope, request.receive, request._send) as (read_stream, write_stream):\n",
    "                await server.run(read_stream, write_stream, server.create_initialization_options())\n",
    "            return Response(status_code=204)  # <â€” important!\n",
    "\n",
    "        starlette_app = Starlette(\n",
    "            debug=True,\n",
    "            routes=[\n",
    "                Route(\"/sse\", endpoint=handle_sse),\n",
    "                Mount(\"/messages/\", app=sse.handle_post_message),\n",
    "            ],\n",
    "        )\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply() \n",
    "\n",
    "        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)  # nosec\n",
    "        \n",
    "    elif transport == \"stdio\":\n",
    "        import anyio\n",
    "        from mcp.server.stdio import stdio_server\n",
    "\n",
    "        async def handle_stdin(stdin: Any | None = None, stdout: Any | None = None) -> None:\n",
    "            async with stdio_server() as (read_stream, write_stream):\n",
    "                await server.run(read_stream, write_stream, server.create_initialization_options())\n",
    "\n",
    "        anyio.run(handle_stdin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84825b",
   "metadata": {},
   "source": [
    "#### Blocking cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0811bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "## this a blocking call - it will block the running further cells\n",
    "asyncio.run(run(transport=\"sse\", port=9090), debug=True)  # Change transport to \"stdio\" if you want to run it in stdio mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbaa97",
   "metadata": {},
   "source": [
    "#### Non-blocking cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb63c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Schedule your long-running coroutine without blocking the cell\n",
    "loop = asyncio.get_event_loop()             # Jupyterâ€™s loop\n",
    "mcp_agent_task = loop.create_task(run(transport=\"sse\", port=9090))\n",
    "mcp_agent_task.set_name(\"mcp-agent\")     # optional, helps with debugging\n",
    "\n",
    "# (optional) surface exceptions instead of failing silently\n",
    "def _report_done(t: asyncio.Task):\n",
    "    try:\n",
    "        t.result()\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"mcp-agent: cancelled\")\n",
    "    except Exception as e:\n",
    "        print(\"mcp-agent: crashed with:\", repr(e))\n",
    "\n",
    "mcp_agent_task.add_done_callback(_report_done)\n",
    "\n",
    "print(\"Started in background:\", mcp_agent_task)\n",
    "# Now you can continue running other cells without waiting for the agent to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de1666",
   "metadata": {},
   "source": [
    "### SSE Transport MCP Server Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams, mcp_server_tools\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "async def run_agent(url, prompt) -> None:\n",
    "    # Create server params for the remote MCP service\n",
    "    server_params = SseServerParams(\n",
    "        url=url,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        timeout=30,  # Connection timeout in seconds\n",
    "    )\n",
    "\n",
    "    # Get all available tools\n",
    "    tools = await mcp_server_tools(server_params)\n",
    "\n",
    "    # Create an agent that can use the translation tool\n",
    "    model_client = AzureOpenAIChatCompletionClient(azure_deployment=openai_model_name, model=openai_model_name,\n",
    "                azure_endpoint=f\"{apim_resource_gateway_url}/{inference_api_path}\",\n",
    "                api_key=apim_subscription_key,\n",
    "                api_version=inference_api_version\n",
    "    )\n",
    "    agent = AssistantAgent(\n",
    "        name=\"weather\",\n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=True,\n",
    "        tools=tools, # type: ignore\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "    )\n",
    "    await Console(\n",
    "        agent.run_stream(task=prompt)\n",
    "    )\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(run_agent(f\"http://127.0.0.1:9090/sse\", \"What's the weather in Lisbon, Cairo and London?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba876f",
   "metadata": {},
   "source": [
    "### Stop the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask it to stop\n",
    "weather_task.cancel()\n",
    "\n",
    "# Let it handle cancellation and swallow the CancelledError\n",
    "await asyncio.gather(weather_task, return_exceptions=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
