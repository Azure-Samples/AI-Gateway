{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è GenAI Everywhere\n",
    "\n",
    "## Gemini + MCP Agents + Content Safety lab\n",
    "![flow](../../images/gemini-mcp-content-safety.gif)\n",
    "\n",
    "Playground to experiment the [Model Context Protocol](https://modelcontextprotocol.io/) with Azure API Management to enable plug & play of tools to LLMs whilst integrating with 3rd party models such as [Gemini models](https://ai.google.dev/gemini-api/docs). \n",
    "This lab includes the following MCP servers:\n",
    "- Basic oncall service: provides a tool to get a list of random people currently on-call with their status and time zone.\n",
    "- Basic weather service: provide tools to get cities for a given country and retrieve random weather information for a specified city.\n",
    "\n",
    "The lab also explores who to enforce Content Safety and Tokens metrics to create a single pane of glass for all your AI Gateway needs.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "‚ñ∂Ô∏è Click `Run All` to execute all steps sequentially, or execute them `Step by Step`...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management) \n",
    "- Obtain a [Gemini API Key](https://aistudio.google.com/apikey) to be able to use Gemini models\n",
    "\n",
    "*important* please DO NOT checkin the Notebook with the API Key still in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "sys.path.insert(1, '../../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"eastus2\"\n",
    "\n",
    "gemini_api_key = \"{OWN API KEY}\"  # Add your Gemini API key here\n",
    "gemini_model = \"gemini-2.5-flash\"  # Change to your desired Gemini model\n",
    "gemini_path = \"gemini/openai\"  # Change to your desired API path\n",
    "\n",
    "apim_sku = \"Basicv2\"  # Change to your desired APIM SKU\n",
    "apim_subscriptions_config = [{\"name\": \"subscription1\", \"displayName\": \"Subscription 1\"}, \n",
    "                             {\"name\": \"subscription2\", \"displayName\": \"Subscription 2\"}, \n",
    "                             {\"name\": \"subscription3\", \"displayName\": \"Subscription 3\"}]\n",
    "\n",
    "build = 0\n",
    "weather_mcp_server_image = \"weather-mcp-server\"\n",
    "weather_mcp_server_src = \"src/weather/mcp-server\"\n",
    "\n",
    "oncall_mcp_server_image = \"oncall-mcp-server\"\n",
    "oncall_mcp_server_src = \"src/oncall/mcp-server\"\n",
    "\n",
    "utils.print_ok('Notebook initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Verify the Azure CLI and the connected Azure subscription\n",
    "\n",
    "The following commands ensure that you have the latest version of the Azure CLI and that the Azure CLI is connected to your Azure subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = utils.run(\"az account show\", \"Retrieved az account\", \"Failed to get the current az account\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    current_user = output.json_data['user']['name']\n",
    "    tenant_id = output.json_data['tenantId']\n",
    "    subscription_id = output.json_data['id']\n",
    "\n",
    "    utils.print_info(f\"Current user: {current_user}\")\n",
    "    utils.print_info(f\"Tenant ID: {tenant_id}\")\n",
    "    utils.print_info(f\"Subscription ID: {subscription_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This lab uses [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep) to declarative define all the resources that will be deployed in the specified resource group. Change the parameters or the [main.bicep](main.bicep) directly to try different configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resource group if doesn't exist\n",
    "utils.create_resource_group(resource_group_name, resource_group_location)\n",
    "\n",
    "# Define the Bicep parameters\n",
    "bicep_parameters = {\n",
    "    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "    \"contentVersion\": \"1.0.0.0\",\n",
    "    \"parameters\": {\n",
    "        \"apimSku\": { \"value\": apim_sku },\n",
    "        \"geminiApiKey\": { \"value\": gemini_api_key },\n",
    "        \"geminiPath\": { \"value\": gemini_path },\n",
    "        \"apimSubscriptionsConfig\": { \"value\": apim_subscriptions_config },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write the parameters to the params.json file\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "# Run the deployment\n",
    "output = utils.run(f\"az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file main.bicep --parameters params.json --verbose\",\n",
    "    f\"Deployment '{deployment_name}' succeeded\", f\"Deployment '{deployment_name}' failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "Retrieve the required outputs from the Bicep deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all of the outputs from the deployment\n",
    "output = utils.run(f\"az deployment group show --name {deployment_name} -g {resource_group_name}\", f\"Retrieved deployment: {deployment_name}\", f\"Failed to retrieve deployment: {deployment_name}\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    log_analytics_id = utils.get_deployment_output(output, 'logAnalyticsWorkspaceId', 'Log Analytics Id')\n",
    "    apim_resource_name = utils.get_deployment_output(output, 'apimResourceName', 'APIM Resource Name')\n",
    "    apim_service_id = utils.get_deployment_output(output, 'apimServiceId', 'APIM Service Id')\n",
    "    apim_resource_gateway_url = utils.get_deployment_output(output, 'apimResourceGatewayURL', 'APIM API Gateway URL')\n",
    "    apim_subscriptions = json.loads(utils.get_deployment_output(output, 'apimSubscriptions').replace(\"\\'\", \"\\\"\"))\n",
    "    for subscription in apim_subscriptions:\n",
    "        subscription_name = subscription['name']\n",
    "        subscription_key = subscription['key']\n",
    "        utils.print_info(f\"Subscription Name: {subscription_name}\")\n",
    "        utils.print_info(f\"Subscription Key: ****{subscription_key[-4:]}\")\n",
    "    api_key = apim_subscriptions[0].get(\"key\") # default api key to the first subscription key\n",
    "    app_insights_name = utils.get_deployment_output(output, 'applicationInsightsName', 'Application Insights Name')\n",
    "    container_registry_name = utils.get_deployment_output(output, 'containerRegistryName', 'Container Registry Name')\n",
    "    weather_containerapp_resource_name = utils.get_deployment_output(output, 'weatherMCPServerContainerAppResourceName', 'Weather Container App Resource Name')\n",
    "    oncall_containerapp_resource_name = utils.get_deployment_output(output, 'oncallMCPServerContainerAppResourceName', 'Oncall Container App Resource Name')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### 4Ô∏è‚É£ Build and deploy the MCP Servers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build = build + 1 # increment the build number\n",
    "\n",
    "utils.run(f\"az acr build --image {weather_mcp_server_image}:v0.{build} --resource-group {resource_group_name} --registry {container_registry_name} --file {weather_mcp_server_src}/Dockerfile {weather_mcp_server_src}/. --no-logs\", \n",
    "          \"Weather MCP Server image was successfully built\", \"Failed to build the Weather MCP Server image\")\n",
    "utils.run(f'az containerapp update -n {weather_containerapp_resource_name} -g {resource_group_name} --image \"{container_registry_name}.azurecr.io/{weather_mcp_server_image}:v0.{build}\"', \n",
    "          \"Weather MCP Server deployment succeeded\", \"Weather MCP Server deployment failed\")\n",
    "\n",
    "utils.run(f\"az acr build --image {oncall_mcp_server_image}:v0.{build} --resource-group {resource_group_name} --registry {container_registry_name} --file {oncall_mcp_server_src}/Dockerfile {oncall_mcp_server_src}/. --no-logs\", \n",
    "          \"Oncall MCP Server image was successfully built\", \"Failed to build the Oncall MCP Server image\")\n",
    "utils.run(f'az containerapp update -n {oncall_containerapp_resource_name} -g {resource_group_name} --image \"{container_registry_name}.azurecr.io/{oncall_mcp_server_image}:v0.{build}\"', \n",
    "          \"Oncall MCP Server deployment succeeded\", \"Oncall MCP Server deployment failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the Content Safety\n",
    "\n",
    "Tip: Use the [tracing tool](../../tools/tracing.ipynb) to track the behavior and troubleshoot the [policy](policy.xml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me how to hurt myself, please?\"}\n",
    "]\n",
    "client = OpenAI(\n",
    "    base_url=f\"{apim_resource_gateway_url}/{gemini_path}\",\n",
    "    api_key=api_key,\n",
    "    #### these additional headers are required for the OpenAI client to work through the AI Gateway\n",
    "    default_headers={\n",
    "        \"api-key\": api_key,\n",
    "    },\n",
    ")\n",
    "try:\n",
    "    response = client.chat.completions.create(model=gemini_model, messages=messages) # type: ignore\n",
    "    print(\"Prompt was not blocked: \", response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='testconnection'></a>\n",
    "### üß™ Test the connection to the MCP servers and List Tools\n",
    "\n",
    "üí° To integrate MCP servers in VS Code, use the MCP server URL  `../sse ` for configuration in GitHub Copilot Agent Mode\n",
    "\n",
    "If the notebook is run again, the JWT validation that gets applied to the policies later on must first be removed. Otherwise, the next calls below will fail as auth is not yet expected to be in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, asyncio, time, requests\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def list_tools(server_url, authorization_header = None):\n",
    "    headers = {\"Authorization\": authorization_header} if authorization_header else None\n",
    "    streams = None\n",
    "    session = None\n",
    "    tools = []\n",
    "    try:\n",
    "        streams_ctx = sse_client(server_url, headers)\n",
    "        streams = await streams_ctx.__aenter__()\n",
    "        session_ctx = ClientSession(streams[0], streams[1])\n",
    "        session = await session_ctx.__aenter__()\n",
    "        await session.initialize()\n",
    "        response = await session.list_tools()\n",
    "        tools = response.tools\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    finally:\n",
    "        # Ensure session and streams are closed if they were opened\n",
    "        if session is not None:\n",
    "            await session_ctx.__aexit__(None, None, None) # type: ignore\n",
    "        if streams is not None:\n",
    "            await streams_ctx.__aexit__(None, None, None) # type: ignore\n",
    "    if tools:\n",
    "        print(f\"‚úÖ Connected to server {server_url}\")\n",
    "        print(\"‚öôÔ∏è Tools:\")\n",
    "        for tool in tools:\n",
    "            print(f\"  - {tool.name}\")\n",
    "            print(f\"     Input Schema: {tool.inputSchema}\")\n",
    "\n",
    "try:    \n",
    "    asyncio.run(list_tools(f\"{apim_resource_gateway_url}/weather/sse\"))\n",
    "    asyncio.run(list_tools(f\"{apim_resource_gateway_url}/oncall/sse\"))\n",
    "finally:\n",
    "    print(f\"‚úÖ Connection closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inspector'></a>\n",
    "### üß™ (optional) Use the [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) for testing and debugging the MCP servers\n",
    "\n",
    "#### Execute the following steps:\n",
    "1. Execute `npx @modelcontextprotocol/inspector` in a terminal\n",
    "2. Open the provided URL in a browser\n",
    "3. Set the transport type as SSE\n",
    "4. Provide the MCP server url and click connect\n",
    "5. Select the \"Tools\" tab to see and run the available tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='functioncalling'></a>\n",
    "### üß™ Run an OpenAI completion with MCP tools\n",
    "\n",
    "üëâ Both the calls to Azure OpenAI and the MCP tools will be managed through Azure API Management.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import json, asyncio\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def call_tool(mcp_session, function_name, function_args):\n",
    "    try:\n",
    "        func_response = await mcp_session.call_tool(function_name, function_args)\n",
    "        func_response_content = func_response.content\n",
    "    except Exception as e:\n",
    "        func_response_content = json.dumps({\"error\": str(e)})\n",
    "    return str(func_response_content)\n",
    "\n",
    "async def run_completion_with_tools(server_url, prompt):\n",
    "    streams = None\n",
    "    session = None\n",
    "    try:\n",
    "        streams_ctx = sse_client(server_url)\n",
    "        streams = await streams_ctx.__aenter__()\n",
    "        session_ctx = ClientSession(streams[0], streams[1])\n",
    "        session = await session_ctx.__aenter__()\n",
    "        await session.initialize()\n",
    "        response = await session.list_tools()\n",
    "        tools = response.tools\n",
    "        print(f\"‚úÖ Connected to server {server_url}\")\n",
    "        openai_tools = [{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"parameters\": tool.inputSchema\n",
    "                },\n",
    "            } for tool in tools]\n",
    "\n",
    "        # Step 1: send the conversation and available functions to the model\n",
    "        print(\"‚ñ∂Ô∏è Step 1: start a completion to identify the appropriate functions to invoke based on the prompt\")\n",
    "        client = OpenAI(\n",
    "            base_url=f\"{apim_resource_gateway_url}/{gemini_path}\",\n",
    "            api_key=api_key,\n",
    "            #### these additional headers are required for the OpenAI client to work through the AI Gateway\n",
    "            default_headers={\n",
    "                \"api-key\": api_key,\n",
    "            },\n",
    "        )\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = client.chat.completions.create(\n",
    "            model=gemini_model,\n",
    "            messages=messages,\n",
    "            tools=openai_tools,\n",
    "        )\n",
    "        response_message = response.choices[0].message\n",
    "        tool_calls = response_message.tool_calls\n",
    "        if tool_calls:\n",
    "            # Step 2: call the function\n",
    "            messages.append(response_message)  # extend conversation with assistant's reply\n",
    "            # send the info for each function call and function response to the model\n",
    "            print(\"‚ñ∂Ô∏è Step 2: call the functions\")\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"   Function Name: {function_name} Function Args: {function_args}\")\n",
    "\n",
    "                function_response = await call_tool(session, function_name, function_args)\n",
    "                # Add the tool response\n",
    "                print(f\"   Function response: {function_response}\")\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": function_response,\n",
    "                    }\n",
    "                )  # extend conversation with function response\n",
    "            print(\"‚ñ∂Ô∏è Step 3: finish with a completion to anwser the user prompt using the function response\")\n",
    "            second_response = client.chat.completions.create(\n",
    "                model='gemini-2.0-flash',\n",
    "                messages=messages,\n",
    "            )  # get a new response from the model where it can see the function response\n",
    "            print(\"üí¨\", second_response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    finally:\n",
    "        if session is not None:\n",
    "            await session_ctx.__aexit__(None, None, None)\n",
    "        if streams is not None:\n",
    "            await streams_ctx.__aexit__(None, None, None)\n",
    "\n",
    "asyncio.run(run_completion_with_tools(f\"{apim_resource_gateway_url}/weather/sse\", \"What's the current weather in Lisbon?\"))\n",
    "asyncio.run(run_completion_with_tools(f\"{apim_resource_gateway_url}/oncall/sse\", \"Who's oncall today?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='autogen'></a>\n",
    "### üß™ Execute an [AutoGen Agent using MCP Tools](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html) via Azure API Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient, OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams, mcp_server_tools\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "async def run_agent(url, prompt) -> None:\n",
    "    # Create server params for the remote MCP service\n",
    "    server_params = SseServerParams(\n",
    "        url=url,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        timeout=30,  # Connection timeout in seconds\n",
    "    )\n",
    "\n",
    "    # Get all available tools\n",
    "    tools = await mcp_server_tools(server_params)\n",
    "\n",
    "    # Create an agent that can use the translation tool\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "                model=gemini_model,\n",
    "                base_url=f\"{apim_resource_gateway_url}/{gemini_path}\",\n",
    "                api_key=api_key,\n",
    "                #### these additional headers are required for the OpenAI client to work through the AI Gateway\n",
    "                default_headers={\n",
    "                    \"api-key\": api_key,\n",
    "                }\n",
    "    )\n",
    "\n",
    "    agent = AssistantAgent(\n",
    "        name=\"weather\",\n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=True,\n",
    "        tools=tools, # type: ignore\n",
    "        system_message=\"You are a helpful yet sarcastic assistant.\",\n",
    "    )\n",
    "    await Console(\n",
    "        agent.run_stream(task=prompt)\n",
    "    )\n",
    "\n",
    "asyncio.run(run_agent(f\"{apim_resource_gateway_url}/weather/sse\", \"What's the weather in Lisbon, Cairo and London?\"))\n",
    "asyncio.run(run_agent(f\"{apim_resource_gateway_url}/oncall/sse\", \"Who's oncall today?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
