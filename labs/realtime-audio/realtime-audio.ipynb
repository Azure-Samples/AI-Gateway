{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Azure OpenAI Realtime Audio lab\n",
    "![flow](../../images/realtime-audio.gif)\n",
    "\n",
    "Playground to try the APIM integration with the [Azure OpenAI Realtime API](https://learn.microsoft.com/en-us/azure/ai-services/openai/realtime-audio-reference) for text and audio.\n",
    "\n",
    "### Result\n",
    "![result](result.png)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "‚ñ∂Ô∏è Click `Run All` to execute all steps sequentially, or execute them `Step by Step`...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management)\n",
    "- Adjust the OpenAI model and version according the [availability by region.](https://learn.microsoft.com/azure/ai-services/openai/concepts/models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "sys.path.insert(1, '../../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"eastus2\"\n",
    "\n",
    "aiservices_config = [{\"name\": \"foundry1\", \"location\": \"eastus2\"}]\n",
    "\n",
    "models_config = [{\"name\": \"gpt-realtime\", \"publisher\": \"OpenAI\", \"version\": \"2025-08-28\", \"sku\": \"GlobalStandard\", \"capacity\": 10}]\n",
    "\n",
    "apim_sku = 'Basicv2'\n",
    "apim_subscriptions_config = [{\"name\": \"subscription1\", \"displayName\": \"Subscription 1\"}]\n",
    "\n",
    "inference_api_path = 'inference'  # path to the inference API in the APIM service\n",
    "inference_api_type = \"websocket\"  \n",
    "inference_api_version = \"2024-10-01-preview\"\n",
    "foundry_project_name = deployment_name\n",
    "\n",
    "utils.print_ok('Notebook initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Verify the Azure CLI and the connected Azure subscription\n",
    "\n",
    "The following commands ensure that you have the latest version of the Azure CLI and that the Azure CLI is connected to your Azure subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = utils.run(\"az account show\", \"Retrieved az account\", \"Failed to get the current az account\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    current_user = output.json_data['user']['name']\n",
    "    tenant_id = output.json_data['tenantId']\n",
    "    subscription_id = output.json_data['id']\n",
    "\n",
    "    utils.print_info(f\"Current user: {current_user}\")\n",
    "    utils.print_info(f\"Tenant ID: {tenant_id}\")\n",
    "    utils.print_info(f\"Subscription ID: {subscription_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This lab uses [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep) to declarative define all the resources that will be deployed in the specified resource group. Change the parameters or the [main.bicep](main.bicep) directly to try different configurations.\n",
    "\n",
    "`openAIModelCapacity` is set intentionally low to `6` (6k tokens per minute) to trigger the retry logic in the load balancer (transparent to the user) as well as the priority failover from priority 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resource group if doesn't exist\n",
    "utils.create_resource_group(resource_group_name, resource_group_location)\n",
    "\n",
    "# Define the Bicep parameters\n",
    "bicep_parameters = {\n",
    "    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "    \"contentVersion\": \"1.0.0.0\",\n",
    "    \"parameters\": {\n",
    "        \"apimSku\": { \"value\": apim_sku },\n",
    "        \"aiServicesConfig\": { \"value\": aiservices_config },\n",
    "        \"modelsConfig\": { \"value\": models_config },\n",
    "        \"apimSubscriptionsConfig\": { \"value\": apim_subscriptions_config },\n",
    "        \"inferenceAPIPath\": { \"value\": inference_api_path },\n",
    "        \"inferenceAPIType\": { \"value\": inference_api_type },\n",
    "        \"foundryProjectName\": { \"value\": foundry_project_name }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write the parameters to the params.json file\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "# Run the deployment\n",
    "output = utils.run(f\"az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file main.bicep --parameters params.json\",\n",
    "    f\"Deployment '{deployment_name}' succeeded\", f\"Deployment '{deployment_name}' failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "Retrieve the required outputs from the Bicep deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all of the outputs from the deployment\n",
    "output = utils.run(f\"az deployment group show --name {deployment_name} -g {resource_group_name}\", f\"Retrieved deployment: {deployment_name}\", f\"Failed to retrieve deployment: {deployment_name}\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    log_analytics_id = utils.get_deployment_output(output, 'logAnalyticsWorkspaceId', 'Log Analytics Id')\n",
    "    apim_service_id = utils.get_deployment_output(output, 'apimServiceId', 'APIM Service Id')\n",
    "    apim_resource_gateway_url = utils.get_deployment_output(output, 'apimResourceGatewayURL', 'APIM API Gateway URL')\n",
    "    apim_subscriptions = json.loads(utils.get_deployment_output(output, 'apimSubscriptions').replace(\"\\'\", \"\\\"\"))\n",
    "    for subscription in apim_subscriptions:\n",
    "        subscription_name = subscription['name']\n",
    "        subscription_key = subscription['key']\n",
    "        utils.print_info(f\"Subscription Name: {subscription_name}\")\n",
    "        utils.print_info(f\"Subscription Key: ****{subscription_key[-4:]}\")\n",
    "    api_key = apim_subscriptions[0].get(\"key\") # default api key to the first subscription key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### 4 Install Python library requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='text'></a>\n",
    "### üß™ Test the Realtime API using just text\n",
    "\n",
    "üëâ Based on this [sample](https://github.com/openai/openai-python/blob/main/examples/realtime/azure_realtime.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from openai import AsyncAzureOpenAI\n",
    "import asyncio, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main() -> None:\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = AsyncAzureOpenAI(\n",
    "            azure_endpoint=f\"{apim_resource_gateway_url}/{inference_api_path}\",\n",
    "            api_key=api_key,\n",
    "            api_version=inference_api_version)\n",
    "    async with client.realtime.connect(model=models_config[0]['name']) as connection:\n",
    "        await connection.session.update(session={\"modalities\": [\"text\"]})  # type: ignore\n",
    "        await connection.conversation.item.create(\n",
    "            item={\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": \"What is the capital of Portugal?\"}],\n",
    "            }\n",
    "        )\n",
    "        await connection.response.create()\n",
    "        async for event in connection:\n",
    "            if event.type == \"response.text.delta\":\n",
    "                print(event.delta, flush=True, end=\"\")\n",
    "            elif event.type == \"response.text.done\":\n",
    "                print()\n",
    "            elif event.type == \"response.done\":\n",
    "                break\n",
    "    await connection.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fastrtc'></a>\n",
    "### üß™ Test the Realtime API using FastRTC + Gradio\n",
    "\n",
    "‚ö° FastRTC is an elegant realtime library communication library to enable you to easily and quickly build RTC application both using websockets and WebRTC.\n",
    "\n",
    "Please ensure you have run the pip command succefully to install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, base64, json, openai\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from openai import AsyncAzureOpenAI\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import HTMLResponse, StreamingResponse\n",
    "from fastrtc import (\n",
    "    AdditionalOutputs,\n",
    "    AsyncStreamHandler,\n",
    "    Stream,\n",
    "    wait_for_item,\n",
    "    UIArgs\n",
    ")\n",
    "from gradio.utils import get_space\n",
    "\n",
    "SAMPLE_RATE = 24000\n",
    "\n",
    "AZURE_OPENAI_API_ENDPOINT = f\"{apim_resource_gateway_url}/{inference_api_path}\"\n",
    "AZURE_OPENAI_API_KEY = api_key\n",
    "AZURE_OPENAI_API_VERSION = inference_api_version\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = models_config[0]['name']\n",
    "SESSION_CONFIG={\n",
    "    \"input_audio_transcription\": {\n",
    "      \"model\": \"whisper-1\"\n",
    "    },\n",
    "    \"turn_detection\": {\n",
    "      \"threshold\": 0.4,\n",
    "      \"silence_duration_ms\": 600,\n",
    "      \"type\": \"server_vad\"\n",
    "    },\n",
    "    \"instructions\": \"Your name is Amy. You're a helpful agent who responds initially with a clam British accent, but also can speak in any language as the user chooses to. Always start the conversation with a cheery hello\",\n",
    "    \"voice\": \"shimmer\",\n",
    "    \"modalities\": [\"text\", \"audio\"] ## required to solicit the initial welcome message\n",
    "    }\n",
    "\n",
    "def on_open(ws):\n",
    "    print(\"Connected to server.\")\n",
    "\n",
    "def on_message(ws, message):\n",
    "    data = json.loads(message)\n",
    "    print(\"Received event:\", json.dumps(data, indent=2))\n",
    "\n",
    "class OpenAIHandler(AsyncStreamHandler):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            expected_layout=\"mono\",\n",
    "            output_sample_rate=SAMPLE_RATE,\n",
    "            output_frame_size=480,  # In this example we choose 480 samples per frame.\n",
    "            input_sample_rate=SAMPLE_RATE,\n",
    "        )\n",
    "        self.connection = None\n",
    "        self.output_queue = asyncio.Queue()\n",
    "\n",
    "    def copy(self):\n",
    "        return OpenAIHandler()\n",
    "\n",
    "    async def welcome(self):\n",
    "        await self.connection.conversation.item.create( # type: ignore\n",
    "            item={\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": \"what's your name?\"}],\n",
    "            }\n",
    "        )\n",
    "        await self.connection.response.create() # type: ignore\n",
    "\n",
    "    async def start_up(self):\n",
    "        \"\"\"\n",
    "        Establish a persistent realtime connection to the Azure OpenAI backend.\n",
    "        The connection is configured for server‚Äêside Voice Activity Detection.\n",
    "        \"\"\"\n",
    "        self.client = openai.AsyncAzureOpenAI(\n",
    "            azure_endpoint=AZURE_OPENAI_API_ENDPOINT,\n",
    "            azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "            api_key=AZURE_OPENAI_API_KEY,\n",
    "            api_version=AZURE_OPENAI_API_VERSION,\n",
    "        )\n",
    "        # When using Azure OpenAI realtime (beta), set the model/deployment identifier\n",
    "        async with self.client.realtime.connect(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT_NAME  # Replace with your deployed realtime model id on Azure OpenAI.\n",
    "        ) as conn:\n",
    "            # Configure the session to use server-based voice activity detection (VAD)\n",
    "            await conn.session.update(session=SESSION_CONFIG) # type: ignore\n",
    "            self.connection = conn\n",
    "\n",
    "            # Uncomment the following line to send a welcome message to the assistant.\n",
    "            # await self.welcome()\n",
    "\n",
    "            async for event in self.connection:\n",
    "                # Handle interruptions\n",
    "                if event.type == \"input_audio_buffer.speech_started\":\n",
    "                    self.clear_queue()\n",
    "                if event.type == \"conversation.item.input_audio_transcription.completed\":\n",
    "                    # This event signals that an input audio transcription is completed.\n",
    "                    await self.output_queue.put(AdditionalOutputs(event))\n",
    "                if event.type == \"response.audio_transcript.done\":\n",
    "                    # This event signals that a response audio transcription is completed.\n",
    "                    await self.output_queue.put(AdditionalOutputs(event))\n",
    "                if event.type == \"response.audio.delta\":\n",
    "                    # For incremental audio output events, decode the delta.\n",
    "                    await self.output_queue.put(\n",
    "                        (\n",
    "                            self.output_sample_rate,\n",
    "                            np.frombuffer(base64.b64decode(event.delta), dtype=np.int16).reshape(1, -1),\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "    async def receive(self, frame: tuple[int, np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Receives an audio frame from the stream and sends it into the realtime API.\n",
    "        The audio data is encoded as Base64 before appending to the connection's input.\n",
    "        \"\"\"\n",
    "        if not self.connection:\n",
    "            return\n",
    "        _, array = frame\n",
    "        array = array.squeeze()\n",
    "        # Encode audio as Base64 string\n",
    "        audio_message = base64.b64encode(array.tobytes()).decode(\"utf-8\")\n",
    "        await self.connection.input_audio_buffer.append(audio=audio_message)  # type: ignore\n",
    "\n",
    "    async def emit(self) -> tuple[int, np.ndarray] | AdditionalOutputs | None:\n",
    "        \"\"\"\n",
    "        Waits for and returns the next output from the output queue.\n",
    "        The output may be an audio chunk or an additional output such as transcription.\n",
    "        \"\"\"\n",
    "        return await wait_for_item(self.output_queue)\n",
    "\n",
    "    async def shutdown(self) -> None: # type: ignore\n",
    "        if self.connection:\n",
    "            await self.connection.close()\n",
    "            self.connection = None\n",
    "\n",
    "def update_chatbot(chatbot: list[dict], content):\n",
    "    \"\"\"\n",
    "    Append the completed transcription to the chatbot messages.\n",
    "    \"\"\"\n",
    "    if content.type == \"conversation.item.input_audio_transcription.completed\":\n",
    "        chatbot.append({\"role\": \"user\", \"content\": content.transcript})\n",
    "    elif content.type == \"response.audio_transcript.done\":\n",
    "        chatbot.append({\"role\": \"assistant\", \"content\": content.transcript})\n",
    "    return chatbot\n",
    "\n",
    "\n",
    "# Create the Gradio Chatbot component for displaying conversation messages.\n",
    "ui_args: UIArgs = UIArgs(\n",
    "    title=\"APIM ‚ù§Ô∏è OpenAI - Contoso Assistant ü§ñ\",\n",
    ")\n",
    "chatbot = gr.Chatbot(type=\"messages\")\n",
    "latest_message = gr.Textbox(type=\"text\", visible=True)\n",
    "\n",
    "# Instantiate the Stream object that uses the OpenAIHandler.\n",
    "stream = Stream(\n",
    "    OpenAIHandler(),\n",
    "    mode=\"send-receive\",\n",
    "    modality=\"audio\",\n",
    "    additional_inputs=[chatbot],\n",
    "    additional_outputs=[chatbot],\n",
    "    additional_outputs_handler=update_chatbot,\n",
    "    ui_args=ui_args,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stream.ui.launch(server_port=7990)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kql'></a>\n",
    "### üîç Display model usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"model_usage\"\n",
    "\n",
    "output = utils.run(f\"az monitor log-analytics query -w {log_analytics_id} --analytics-query \\\"{query}\\\"\", \"Retrieved log analytics query output\", \"Failed to retrieve log analytics query output\") \n",
    "if output.success and output.json_data:\n",
    "    table = output.json_data\n",
    "    display(pd.DataFrame(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
