{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Azure OpenAI Realtime Audio lab\n",
    "\n",
    "Playground to try the APIM integration with Azure OpenAI realtime Audio.\n",
    "\n",
    "\n",
    "### Result\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with Contributor permissions\n",
    "- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management)\n",
    "- Adjust the OpenAI model and version according the [availability by region.](https://learn.microsoft.com/azure/ai-services/openai/concepts/models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "sys.path.insert(1, '../../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"eastus2\" \n",
    "\n",
    "apim_sku = 'Basicv2'\n",
    "\n",
    "# gpt-4o-realtime and gpt-4o-mini-realtime are only available for eastuse2 or swedencentral\n",
    "openai_resources = [\n",
    "    {\"name\": \"openai1\", \"location\": \"eastus2\"},\n",
    "]\n",
    "\n",
    "openai_deployment_name = \"gpt-4o-realtime-preview\"\n",
    "openai_model_name = \"gpt-4o-realtime-preview\"\n",
    "openai_model_version = \"2024-12-17\"\n",
    "openai_model_capacity = 6\n",
    "openai_model_sku = 'GlobalStandard'\n",
    "openai_api_version = \"2024-10-01-preview\"\n",
    "\n",
    "utils.print_ok('Notebook initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Verify the Azure CLI and the connected Azure subscription\n",
    "\n",
    "The following commands ensure that you have the latest version of the Azure CLI and that the Azure CLI is connected to your Azure subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = utils.run(\"az account show\", \"Retrieved az account\", \"Failed to get the current az account\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    current_user = output.json_data['user']['name']\n",
    "    tenant_id = output.json_data['tenantId']\n",
    "    subscription_id = output.json_data['id']\n",
    "\n",
    "    utils.print_info(f\"Current user: {current_user}\")\n",
    "    utils.print_info(f\"Tenant ID: {tenant_id}\")\n",
    "    utils.print_info(f\"Subscription ID: {subscription_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This lab uses [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep) to declarative define all the resources that will be deployed in the specified resource group. Change the parameters or the [main.bicep](main.bicep) directly to try different configurations.\n",
    "\n",
    "`openAIModelCapacity` is set intentionally low to `6` (6k tokens per minute) to trigger the retry logic in the load balancer (transparent to the user) as well as the priority failover from priority 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resource group if doesn't exist\n",
    "utils.create_resource_group(resource_group_name, resource_group_location)\n",
    "\n",
    "# Define the Bicep parameters\n",
    "bicep_parameters = {\n",
    "    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "    \"contentVersion\": \"1.0.0.0\",\n",
    "    \"parameters\": {\n",
    "        \"apimSku\": { \"value\": apim_sku },\n",
    "        \"openAIConfig\": { \"value\": openai_resources },\n",
    "        \"openAIDeploymentName\": { \"value\": openai_deployment_name },\n",
    "        \"openAIModelName\": { \"value\": openai_model_name },\n",
    "        \"openAIModelVersion\": { \"value\": openai_model_version },\n",
    "        \"openAIModelCapacity\": { \"value\": openai_model_capacity },\n",
    "        \"openAIModelSKU\": { \"value\": openai_model_sku },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write the parameters to the params.json file\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "# Run the deployment\n",
    "output = utils.run(f\"az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file main.bicep --parameters params.json\",\n",
    "    f\"Deployment '{deployment_name}' succeeded\", f\"Deployment '{deployment_name}' failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "Retrieve the required outputs from the Bicep deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all of the outputs from the deployment\n",
    "output = utils.run(f\"az deployment group show --name {deployment_name} -g {resource_group_name}\", f\"Retrieved deployment: {deployment_name}\", f\"Failed to retrieve deployment: {deployment_name}\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    apim_service_id = utils.get_deployment_output(output, 'apimServiceId', 'APIM Service Id')\n",
    "    apim_resource_gateway_url = utils.get_deployment_output(output, 'apimResourceGatewayURL', 'APIM API Gateway URL')\n",
    "    apim_subscription_key = utils.get_deployment_output(output, 'apimSubscriptionKey', 'APIM Subscription Key (masked)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the Realtime API using FastRTC + Gradio\n",
    "FastRTC is an elegant realtime library communication library to enable you to easily and quickly build RTC application both using websockets and WebRTC.\n",
    "\n",
    "Please ensure you have run the pip command succefully to install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import base64\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import AsyncAzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import HTMLResponse, StreamingResponse\n",
    "from fastrtc import (\n",
    "    AdditionalOutputs,\n",
    "    AsyncStreamHandler,\n",
    "    Stream,\n",
    "    get_twilio_turn_credentials,\n",
    "    wait_for_item,\n",
    ")\n",
    "from gradio.utils import get_space\n",
    "from openai.types.beta.realtime import ResponseAudioTranscriptDoneEvent\n",
    "\n",
    "SAMPLE_RATE = 24000\n",
    "\n",
    "AZURE_OPENAI_API_ENDPOINT = apim_resource_gateway_url + \"/rt-audio\"\n",
    "print(AZURE_OPENAI_API_ENDPOINT)\n",
    "AZURE_OPENAI_API_KEY = apim_subscription_key\n",
    "AZURE_OPENAI_API_VERSION = openai_api_version\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = openai_deployment_name\n",
    "SESSION_CONFIG={\n",
    "    \"input_audio_transcription\": {\n",
    "      \"model\": \"whisper-1\"\n",
    "    },\n",
    "    \"turn_detection\": {\n",
    "      \"threshold\": 0.4,\n",
    "      \"silence_duration_ms\": 600,\n",
    "      \"type\": \"server_vad\"\n",
    "    },\n",
    "    \"instructions\": \"Your name is Amy. You're a helpful sarcastic agent who responds initially with a clam British accent, but also can speak in any language as the user chooses to. Always start the conversation with a cheery hello\",\n",
    "    \"voice\": \"shimmer\",\n",
    "    \"modalities\": [\"text\", \"audio\"] ## required to solicit the initial welcome message\n",
    "    }\n",
    "\n",
    "def on_open(ws):\n",
    "    print(\"Connected to server.\")\n",
    "\n",
    "def on_message(ws, message):\n",
    "    data = json.loads(message)\n",
    "    print(\"Received event:\", json.dumps(data, indent=2))\n",
    "\n",
    "class OpenAIHandler(AsyncStreamHandler):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            expected_layout=\"mono\",\n",
    "            output_sample_rate=SAMPLE_RATE,\n",
    "            output_frame_size=480,  # In this example we choose 480 samples per frame.\n",
    "            input_sample_rate=SAMPLE_RATE,\n",
    "        )\n",
    "        self.connection = None\n",
    "        self.output_queue = asyncio.Queue()\n",
    "\n",
    "    def copy(self):\n",
    "        return OpenAIHandler()\n",
    "\n",
    "    async def welcome(self):\n",
    "        await self.connection.conversation.item.create(\n",
    "            item={\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": \"what's your name?\"}],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        await self.connection.response.create()\n",
    "\n",
    "    async def start_up(self):\n",
    "        \"\"\"\n",
    "        Establish a persistent realtime connection to the Azure OpenAI backend.\n",
    "        The connection is configured for server‚Äêside Voice Activity Detection.\n",
    "        \"\"\"\n",
    "        self.client = openai.AsyncAzureOpenAI(\n",
    "            azure_endpoint=AZURE_OPENAI_API_ENDPOINT,\n",
    "            azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "            api_key=AZURE_OPENAI_API_KEY,\n",
    "            api_version=AZURE_OPENAI_API_VERSION,\n",
    "        )\n",
    "        # When using Azure OpenAI realtime (beta), set the model/deployment identifier\n",
    "        async with self.client.beta.realtime.connect(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT_NAME  # Replace with your deployed realtime model id on Azure OpenAI.\n",
    "        ) as conn:\n",
    "            # Configure the session to use server-based voice activity detection (VAD)\n",
    "            await conn.session.update(session=SESSION_CONFIG)\n",
    "            self.connection = conn\n",
    "            await self.welcome()\n",
    "            async for event in self.connection:\n",
    "                # Handle interruptions\n",
    "                if event.type == \"input_audio_buffer.speech_started\":\n",
    "                    self.clear_queue()\n",
    "                if event.type == \"response.audio_transcript.done\":\n",
    "                    # This event signals that an audio transcription is completed.\n",
    "                    await self.output_queue.put(AdditionalOutputs(event))\n",
    "                if event.type == \"response.audio.delta\":\n",
    "                    # For incremental audio output events, decode the delta.\n",
    "                    await self.output_queue.put(\n",
    "                        (\n",
    "                            self.output_sample_rate,\n",
    "                            np.frombuffer(base64.b64decode(event.delta), dtype=np.int16).reshape(1, -1),\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "    async def receive(self, frame: tuple[int, np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Receives an audio frame from the stream and sends it into the realtime API.\n",
    "        The audio data is encoded as Base64 before appending to the connection's input.\n",
    "        \"\"\"\n",
    "        if not self.connection:\n",
    "            return\n",
    "        _, array = frame\n",
    "        array = array.squeeze()\n",
    "        # Encode audio as Base64 string\n",
    "        audio_message = base64.b64encode(array.tobytes()).decode(\"utf-8\")\n",
    "        await self.connection.input_audio_buffer.append(audio=audio_message)  # type: ignore\n",
    "\n",
    "    async def emit(self) -> tuple[int, np.ndarray] | AdditionalOutputs | None:\n",
    "        \"\"\"\n",
    "        Waits for and returns the next output from the output queue.\n",
    "        The output may be an audio chunk or an additional output such as transcription.\n",
    "        \"\"\"\n",
    "        return await wait_for_item(self.output_queue)\n",
    "\n",
    "    async def shutdown(self) -> None:\n",
    "        if self.connection:\n",
    "            await self.connection.close()\n",
    "            self.connection = None\n",
    "\n",
    "def update_chatbot(chatbot: list[dict], response: ResponseAudioTranscriptDoneEvent):\n",
    "    \"\"\"\n",
    "    Append the completed transcription (from Azure OpenAI) to the chatbot messages.\n",
    "    \"\"\"\n",
    "    chatbot.append({\"role\": \"assistant\", \"content\": response.transcript})\n",
    "    return chatbot\n",
    "\n",
    "\n",
    "# Create the Gradio Chatbot component for displaying conversation messages.\n",
    "chatbot = gr.Chatbot(type=\"messages\")\n",
    "latest_message = gr.Textbox(type=\"text\", visible=True)\n",
    "\n",
    "# Instantiate the Stream object that uses the OpenAIHandler.\n",
    "stream = Stream(\n",
    "    OpenAIHandler(),\n",
    "    mode=\"send-receive\",\n",
    "    modality=\"audio\",\n",
    "    additional_inputs=[chatbot],\n",
    "    additional_outputs=[chatbot],\n",
    "    additional_outputs_handler=update_chatbot,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stream.ui.launch(server_name=\"0.0.0.0\", server_port=7860, share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
