{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Backend pool Load Balancing lab\n",
    "![flow](../../images/backend-pool-load-balancing.gif)\n",
    "\n",
    "Playground to try the built-in load balancing [backend pool functionality of APIM](https://learn.microsoft.com/azure/api-management/backends?tabs=bicep) to either a list of Azure OpenAI endpoints.\n",
    "\n",
    "Notes:\n",
    "- The backend pool uses round-robin by default\n",
    "- Priority and weight-based routing are also supported: Adjust the `priority` (the lower the number, the higher the priority) and `weight` parameters in the `openai_resources` variable\n",
    "- The `retry` API Management policy initiates a retry to an available backend if an HTTP 429 status code is encountered\n",
    "\n",
    "### Result\n",
    "![result](result.png)\n",
    "\n",
    "### TOC\n",
    "- [0Ô∏è‚É£ Initialize notebook variables](#0)\n",
    "- [1Ô∏è‚É£ Create the Azure Resource Group](#1)\n",
    "- [2Ô∏è‚É£ Create deployment using ü¶æ Bicep](#2)\n",
    "- [3Ô∏è‚É£ Get the deployment outputs](#3)\n",
    "- [üß™ Test the API using a direct HTTP call](#requests)\n",
    "- [üîç Analyze Load Balancing results](#plot)\n",
    "- [üß™ Test the API using the Azure OpenAI Python SDK](#sdk)\n",
    "- [üóëÔ∏è Clean up resources](#clean)\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.9 or later version](https://www.python.org/) installed\n",
    "- [Pandas Library](https://pandas.pydata.org/) and matplotlib installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with Contributor permissions\n",
    "- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management) \n",
    "- Adjust the OpenAI model and version according the [availability by region.](https://learn.microsoft.com/azure/ai-services/openai/concepts/models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"westeurope\"\n",
    "\n",
    "openai_resources = [\n",
    "    {\"name\": \"openai1\", \"location\": \"uksouth\", \"priority\": 1, \"weight\": 80},\n",
    "    {\"name\": \"openai2\", \"location\": \"swedencentral\", \"priority\": 1, \"weight\": 10},\n",
    "    {\"name\": \"openai3\", \"location\": \"francecentral\", \"priority\": 1, \"weight\": 10}\n",
    "]\n",
    "\n",
    "openai_model_name = \"gpt-35-turbo\"\n",
    "openai_model_version = \"0613\"\n",
    "openai_deployment_name = \"gpt-35-turbo\"\n",
    "openai_api_version = \"2024-02-01\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Create the Azure Resource Group\n",
    "All resources deployed in this lab will be created in the specified resource group. Skip this step if you want to use an existing resource group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../shared/snippets/create-az-resource-group.py\n",
    "\n",
    "# type: ignore\n",
    "\n",
    "import datetime\n",
    "\n",
    "resource_group_stdout = ! az group create --name {resource_group_name} --location {resource_group_location}\n",
    "\n",
    "if resource_group_stdout.n.startswith(\"ERROR\"):\n",
    "    print(resource_group_stdout)\n",
    "else:\n",
    "    print(f\"‚úÖ Azure Resource Group {resource_group_name} created ‚åö {datetime.datetime.now().time()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This lab uses [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep) to declarative define all the resources that will be deployed. Change the parameters or the [main.bicep](main.bicep) directly to try different configurations. \n",
    "\n",
    "`openAIModelCapacity` is set intentionally low to `8` (8k tokens per minute) in _main.bicep_ to showcase the retry logic in the load balancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../shared/snippets/create-az-deployment.py\n",
    "\n",
    "# type: ignore\n",
    "\n",
    "import json\n",
    "\n",
    "backend_id = \"openai-backend-pool\" if len(openai_resources) > 1 else openai_resources[0].get(\"name\")\n",
    "\n",
    "with open(\"policy.xml\", 'r') as policy_xml_file:\n",
    "    policy_xml = policy_xml_file.read()\n",
    "\n",
    "    if \"{backend-id}\" in policy_xml:\n",
    "        policy_xml = policy_xml.replace(\"{backend-id}\", backend_id)\n",
    "\n",
    "    if \"{aad-client-application-id}\" in policy_xml:\n",
    "        policy_xml = policy_xml.replace(\"{aad-client-application-id}\", client_id)\n",
    "\n",
    "    if \"{aad-tenant-id}\" in policy_xml:\n",
    "        policy_xml = policy_xml.replace(\"{aad-tenant-id}\", tenant_id)\n",
    "\n",
    "    policy_xml_file.close()\n",
    "open(\"policy-updated.xml\", 'w').write(policy_xml)\n",
    "\n",
    "bicep_parameters = {\n",
    "    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "    \"contentVersion\": \"1.0.0.0\",\n",
    "    \"parameters\": {\n",
    "        \"openAIConfig\": { \"value\": openai_resources },\n",
    "        \"openAIDeploymentName\": { \"value\": openai_deployment_name },\n",
    "        \"openAIModelName\": { \"value\": openai_model_name },\n",
    "        \"openAIModelVersion\": { \"value\": openai_model_version },\n",
    "        \"openAIAPIVersion\": { \"value\": openai_api_version }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "! az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file \"main.bicep\" --parameters \"params.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "We are now at the stage where we only need to retrieve the gateway URL and the subscription before we are ready for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../shared/snippets/deployment-outputs.py\n",
    "# type: ignore\n",
    "\n",
    "# Obtain all of the outputs from the deployment\n",
    "stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs -o json\n",
    "outputs = json.loads(stdout.n)\n",
    "\n",
    "# Extract the individual properties\n",
    "apim_service_id = outputs.get('apimServiceId', {}).get('value', '')\n",
    "apim_subscription_key = outputs.get('apimSubscriptionKey', {}).get('value', '')\n",
    "apim_subscription1_key = outputs.get('apimSubscription1Key', {}).get('value', '')\n",
    "apim_subscription2_key = outputs.get('apimSubscription2Key', {}).get('value', '')\n",
    "apim_subscription3_key = outputs.get('apimSubscription3Key', {}).get('value', '')\n",
    "apim_resource_gateway_url = outputs.get('apimResourceGatewayURL', {}).get('value', '')\n",
    "workspace_id = outputs.get('logAnalyticsWorkspaceId', {}).get('value', '')\n",
    "app_id = outputs.get('applicationInsightsAppId', {}).get('value', '')\n",
    "function_app_resource_name = outputs.get('functionAppResourceName', {}).get('value', '')\n",
    "cosmosdb_connection_string = outputs.get('cosmosDBConnectionString', {}).get('value', '')\n",
    "\n",
    "# Print the extracted properties if they are not empty\n",
    "if apim_service_id:\n",
    "    print(f\"üëâüèª APIM Service Id: {apim_service_id}\")\n",
    "\n",
    "if apim_subscription_key:\n",
    "    print(f\"üëâüèª APIM Subscription Key (masked): ****{apim_subscription_key[-4:]}\")\n",
    "\n",
    "if apim_subscription1_key:\n",
    "    print(f\"üëâüèª APIM Subscription Key 1 (masked): ****{apim_subscription1_key[-4:]}\")\n",
    "\n",
    "if apim_subscription2_key:\n",
    "    print(f\"üëâüèª APIM Subscription Key 2 (masked): ****{apim_subscription2_key[-4:]}\")\n",
    "\n",
    "if apim_subscription3_key:\n",
    "    print(f\"üëâüèª APIM Subscription Key 3 (masked): ****{apim_subscription3_key[-4:]}\")\n",
    "\n",
    "if apim_resource_gateway_url:\n",
    "    print(f\"üëâüèª APIM API Gateway URL: {apim_resource_gateway_url}\")\n",
    "\n",
    "if workspace_id:\n",
    "    print(f\"üëâüèª Workspace ID: {workspace_id}\")\n",
    "\n",
    "if app_id:\n",
    "    print(f\"üëâüèª App ID: {app_id}\")\n",
    "\n",
    "if function_app_resource_name:\n",
    "    print(f\"üëâüèª Function Name: {function_app_resource_name}\")\n",
    "\n",
    "if cosmosdb_connection_string:\n",
    "    print(f\"üëâüèª Cosmos DB Connection String: {cosmosdb_connection_string}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n",
    "Requests is an elegant and simple HTTP library for Python that will be used here to make raw API requests and inspect the responses. \n",
    "\n",
    "You will not see HTTP 429s returned as API Management's `retry` policy will select an available backend. If no backends are viable, an HTTP 503 will be returned.\n",
    "\n",
    "Tip: Use the [tracing tool](../../tools/tracing.ipynb) to track the behavior of the backend pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../shared/snippets/api-http-requests.py\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "runs = 10\n",
    "sleep_time_ms = 100\n",
    "url = f\"{apim_resource_gateway_url}/openai/deployments/{openai_deployment_name}/chat/completions?api-version={openai_api_version}\"\n",
    "api_runs = []\n",
    "messages = {\"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "# Initialize a session for connection pooling\n",
    "session = requests.Session()\n",
    "# Set default headers\n",
    "session.headers.update({'api-key': apim_subscription_key})\n",
    "\n",
    "try:\n",
    "    for i in range(runs):\n",
    "        print(f\"‚ñ∂Ô∏è Run {i+1}/{runs}:\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = session.post(url, json = messages)\n",
    "        response_time = time.time() - start_time\n",
    "        print(f\"‚åö {response_time:.2f} seconds\")\n",
    "\n",
    "        # Check the response status code and apply formatting\n",
    "        if 200 <= response.status_code < 300:\n",
    "            status_code_str = f\"\\x1b[1;32m{response.status_code} - {response.reason}\\x1b[0m\" # Bold and green\n",
    "        elif response.status_code >= 400:\n",
    "            status_code_str = f\"\\x1b[1;31m{response.status_code} - {response.reason}\\x1b[0m\" # Bold and red\n",
    "        else:\n",
    "            status_code_str = str(response.status_code)  # No formatting\n",
    "\n",
    "        # Print the response status with the appropriate formatting\n",
    "        print(f\"Response status: {status_code_str}\")\n",
    "        print(f\"Response headers: {json.dumps(dict(response.headers), indent = 4)}\")\n",
    "\n",
    "        if \"x-ms-region\" in response.headers:\n",
    "            print(f\"x-ms-region: \\x1b[1;32m{response.headers.get(\"x-ms-region\")}\\x1b[0m\") # this header is useful to determine the region of the backend that served the request\n",
    "            api_runs.append((response_time, response.headers.get(\"x-ms-region\")))\n",
    "\n",
    "        if (response.status_code == 200):\n",
    "            data = json.loads(response.text)\n",
    "            print(f\"Token usage: {json.dumps(dict(data.get(\"usage\")), indent = 4)}\\n\")\n",
    "            print(f\"üí¨ {data.get(\"choices\")[0].get(\"message\").get(\"content\")}\\n\")\n",
    "        else:\n",
    "            print(response.text)\n",
    "\n",
    "        time.sleep(sleep_time_ms/1000)\n",
    "finally:\n",
    "    # Close the session to release the connection\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Load Balancing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle as pltRectangle\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 7]\n",
    "df = pd.DataFrame(api_runs, columns = ['Response Time', 'Region'])\n",
    "df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "# Define a color map for each region\n",
    "color_map = {'UK South': 'lightpink', 'France Central': 'lightblue', 'Sweden Central': 'lightyellow', 'Region3': 'red', 'Region4': 'orange'}  # Add more regions and colors as needed\n",
    "\n",
    "# Plot the dataframe with colored bars\n",
    "ax = df.plot(kind = 'bar', x = 'Run', y = 'Response Time', color = [color_map.get(region, 'gray') for region in df['Region']], legend = False)\n",
    "\n",
    "# Add legend\n",
    "legend_labels = [pltRectangle((0, 0), 1, 1, color = color_map.get(region, 'gray')) for region in df['Region'].unique()]\n",
    "ax.legend(legend_labels, df['Region'].unique())\n",
    "\n",
    "plt.title('Load Balancing results')\n",
    "plt.xlabel('Run #')\n",
    "plt.ylabel('Response Time')\n",
    "plt.xticks(rotation = 0)\n",
    "\n",
    "average = df['Response Time'].mean()\n",
    "plt.axhline(y = average, color = 'r', linestyle = '--', label = f'Average: {average:.2f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "\n",
    "Repeat the same test using the Python SDK to ensure compatibility. Note that we do not know what region served the response; we only see that we obtained a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../shared/snippets/openai-api-requests.py\n",
    "\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "runs = 10\n",
    "sleep_time_ms = 100\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = apim_resource_gateway_url,\n",
    "    api_key = apim_subscription_key,\n",
    "    api_version = openai_api_version\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]\n",
    "\n",
    "for i in range(runs):\n",
    "    print(f\"‚ñ∂Ô∏è Run {i+1}/{runs}:\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model = openai_model_name, messages = messages) # type: ignore\n",
    "    response_time = time.time() - start_time\n",
    "    print(f\"‚åö {response_time:.2f} seconds\")\n",
    "    if response.usage:\n",
    "        print(f\"Token usage: Total tokens: {response.usage.total_tokens} (Prompt tokens: {response.usage.prompt_tokens} & Completion tokens: {response.usage.completion_tokens})\")\n",
    "    print(f\"üí¨ {response.choices[0].message.content}\\n\")\n",
    "\n",
    "    time.sleep(sleep_time_ms/1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
