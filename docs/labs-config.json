[
  {
    "id": "access-controlling",
    "name": "Access Controlling",
    "architectureDiagram": "images/access-controlling.gif",
    "categories": [
      "Governance & Responsible AI",
      "Platform Capabilities"
    ],
    "services": [
      "Entra ID",
      "Managed Identity"
    ],
    "shortDescription": "Implement authentication and authorization for AI Gateway APIs.",
    "detailedDescription": "Secure your AI Gateway with comprehensive access control mechanisms. This lab covers implementing OAuth 2.0, Azure AD authentication, API keys, and managed identities. Learn to set up subscription-based access, implement JWT validation, configure CORS policies, and create different access tiers for your AI services.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/access-controlling"
  },
  {
    "id": "ai-agent-service",
    "name": "AI Agent Service",
    "architectureDiagram": "images/ai-agent-service-small.gif",
    "categories": [
      "AI Agents"
    ],
    "services": [
      "Azure Functions",
      "Azure OpenAI"
    ],
    "shortDescription": "Build and orchestrate AI agents with specialized capabilities.",
    "detailedDescription": "Create sophisticated AI agent services that can perform specialized tasks and orchestrate multiple agents. This lab demonstrates how to build agents with specific capabilities (like weather lookup, order placement, or data analysis), use function calling to enable agent actions, and coordinate multiple agents through Azure API Management for complex workflows.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/ai-agent-service"
  },
  {
    "id": "ai-foundry-deepseek",
    "name": "AI Foundry Deepseek",
    "architectureDiagram": "images/ai-foundry-deepseek.gif",
    "categories": [
      "Models Usage",
      "Platform Capabilities"
    ],
    "services": [
      "Azure AI Foundry",
      "Deepseek R1"
    ],
    "shortDescription": "Experiment with Deepseek R1 model via Azure AI Foundry's Model Inference API.",
    "detailedDescription": "Playground to try the Deepseek R1 model via the AI Model Inference from Azure AI Foundry. This lab uses the Azure AI Model Inference API and two APIM LLM policies: llm-token-limit and llm-emit-token-metric to manage and monitor token usage while accessing cutting-edge reasoning models.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/ai-foundry-deepseek"
  },
  {
    "id": "ai-foundry-sdk",
    "name": "AI Foundry SDK",
    "architectureDiagram": "images/ai-foundry-sdk.gif",
    "categories": [
      "Platform Capabilities",
      "Models Usage"
    ],
    "services": [
      "Azure AI Foundry"
    ],
    "shortDescription": "Use Azure AI Foundry SDK with API Management for enhanced AI applications.",
    "detailedDescription": "Learn to leverage the Azure AI Foundry SDK in combination with Azure API Management for building sophisticated AI applications. This lab demonstrates how to use the SDK for model deployment, prompt flow execution, and evaluation, while routing traffic through API Management for governance, security, and monitoring.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/ai-foundry-sdk"
  },
  {
    "id": "aws-bedrock",
    "name": "AWS Bedrock Integration",
    "architectureDiagram": "images/aws-bedrock.gif",
    "categories": [
      "Models Usage",
      "Platform Capabilities"
    ],
    "services": [
      "Multi-Cloud AI"
    ],
    "shortDescription": "Access AWS Bedrock models through Azure API Management gateway.",
    "detailedDescription": "Build a multi-cloud AI architecture by integrating AWS Bedrock models with Azure API Management. This lab demonstrates how to expose Claude, Llama, and other Bedrock models through your Azure gateway, implement unified authentication and rate limiting across cloud providers, and create a consistent API interface for multi-cloud AI consumption.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/aws-bedrock"
  },
  {
    "id": "backend-pool-load-balancing",
    "name": "Backend Pool Load Balancing",
    "architectureDiagram": "images/backend-pool-load-balancing.gif",
    "categories": [
      "Platform Capabilities"
    ],
    "services": [
      "Azure AI Foundry",
      "Azure OpenAI"
    ],
    "shortDescription": "Built-in load balancing using APIM backend pool functionality across multiple Azure AI Foundry endpoints.",
    "detailedDescription": "This lab demonstrates the built-in load balancing backend pool functionality of Azure API Management to distribute traffic across a list of Azure AI Foundry endpoints. This is a typical prioritized PTU with fallback consumption scenario. The lab specifically showcases how a priority 1 (highest) backend is exhausted before gracefully falling back to two equally-weighted priority 2 backends.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/backend-pool-load-balancing"
  },
  {
    "id": "backend-pool-load-balancing-tf",
    "name": "(Terraform) Backend Pool Load Balancing (Terraform)",
    "architectureDiagram": "images/backend-pool-load-balancing.gif",
    "categories": [
      "Platform Capabilities"
    ],
    "services": [
      "Azure OpenAI"
    ],
    "shortDescription": "Built-in load balancing with Terraform - prioritized PTU with fallback consumption scenario.",
    "detailedDescription": "Playground to try the built-in load balancing backend pool functionality of APIM to a list of Azure OpenAI endpoints using Terraform for infrastructure as code. This is a typical prioritized PTU with fallback consumption scenario. The lab specifically showcases how a priority 1 (highest) backend is exhausted before gracefully falling back to two equally-weighted priority 2 backends.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/backend-pool-load-balancing-tf"
  },
  {
    "id": "built-in-logging",
    "name": "Built-in Logging",
    "architectureDiagram": "images/built-in-logging.gif",
    "categories": [
      "Platform Capabilities"
    ],
    "services": [
      "Application Insights",
      "Log Analytics"
    ],
    "shortDescription": "Enable comprehensive logging for debugging and monitoring AI Gateway operations.",
    "detailedDescription": "Configure built-in logging capabilities in the AI Gateway to capture detailed request and response data. This lab covers how to set up logging to Application Insights and Log Analytics, configure log levels, implement custom logging policies, and build queries to analyze API traffic patterns and troubleshoot issues.",
    "authors": [
      "vieiraae"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/built-in-logging",
    "tags": [],
    "lastCommitDate": "2026-02-04T12:50:02.820Z"
  },
  {
    "id": "content-safety",
    "name": "Content Safety",
    "architectureDiagram": "images/content-safety.gif",
    "categories": [
      "Governance & Responsible AI"
    ],
    "services": [
      "Azure AI Content Safety",
      "Azure OpenAI"
    ],
    "shortDescription": "Implement content filtering and safety checks for AI-generated responses.",
    "detailedDescription": "This lab demonstrates how to implement Azure AI Content Safety to filter and moderate content in both requests and responses. Learn how to protect your applications from harmful content including hate speech, violence, self-harm, and sexual content using Azure's advanced content moderation capabilities integrated with Azure API Management.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/content-safety"
  },
  {
    "id": "finops-framework",
    "name": "FinOps Framework",
    "architectureDiagram": "images/finops-framework.gif",
    "categories": [
      "Governance & Responsible AI",
      "Platform Capabilities"
    ],
    "services": [
      "Azure Monitor",
      "Cost Management",
      "FinOps"
    ],
    "shortDescription": "Implement comprehensive cost tracking and optimization for AI services.",
    "detailedDescription": "Implement a complete FinOps framework for managing AI service costs. This lab covers setting up detailed cost tracking, creating chargeback models for different teams or customers, implementing budget alerts, optimizing token usage, and building dashboards for cost visibility. Learn to balance performance, quality, and cost in your AI deployments.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/finops-framework"
  },
  {
    "id": "function-calling",
    "name": "Function Calling",
    "architectureDiagram": "images/function-calling.gif",
    "categories": [
      "Models Usage",
      "Knowledge & Tools"
    ],
    "services": [
      "Azure Functions",
      "Azure OpenAI"
    ],
    "shortDescription": "Enable AI models to call external functions and APIs for enhanced capabilities.",
    "detailedDescription": "Explore the function calling capabilities of Azure OpenAI models integrated through Azure API Management. This lab demonstrates how to define functions, enable AI models to intelligently call external APIs and services, and process the results. Learn to build AI agents that can interact with real-world systems and data sources.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/function-calling"
  },
  {
    "id": "gemini-mcp-agents",
    "name": "Gemini MCP Agents",
    "architectureDiagram": "images/gemini-mcp-agents.gif",
    "categories": [
      "AI Agents",
      "Models Usage"
    ],
    "services": [
      "MCP",
      "Multi-Cloud AI"
    ],
    "shortDescription": "Integrate Google Gemini models with MCP-based agents through Azure API Management.",
    "detailedDescription": "Learn to integrate Google Gemini models with Model Context Protocol (MCP) agents through Azure API Management. This lab demonstrates multi-cloud AI architectures, showing how to expose Gemini's capabilities alongside Azure OpenAI, implement MCP servers for tool access, and manage cross-cloud AI services through a unified gateway.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/gemini-mcp-agents"
  },
  {
    "id": "image-generation",
    "name": "Image Generation",
    "architectureDiagram": "images/image-gen.gif",
    "categories": [
      "Models Usage"
    ],
    "services": [
      "Image Generation",
      "Azure OpenAI"
    ],
    "shortDescription": "Generate images using DALL-E through Azure API Management gateway.",
    "detailedDescription": "Learn how to expose and manage Azure OpenAI's DALL-E image generation capabilities through Azure API Management. This lab covers setting up the gateway for image generation requests, implementing size and quality controls, managing costs through rate limiting, and handling image responses efficiently.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/image-generation"
  },
  {
    "id": "mcp-a2a-agents",
    "name": "MCP A2A Agents",
    "architectureDiagram": "images/mcp-a2a-agents.gif",
    "categories": [
      "AI Agents",
      "Knowledge & Tools"
    ],
    "services": [
      "MCP",
      "Azure Container Apps",
      "Semantic Kernel",
      "Autogen",
      "A2A Protocol"
    ],
    "shortDescription": "Multi-agent system using Agent-to-Agent protocol with MCP servers and heterogeneous orchestrators.",
    "detailedDescription": "Playground to experiment with A2A-enabled agents with Model Context Protocol through Azure API Management. Demonstrates heterogeneous multi-agent systems with agents built using Semantic Kernel and Autogen, communicating through APIM for authentication and authorization. Includes MCP servers for oncall service, weather, and GitHub issues with OAuth integration.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-a2a-agents"
  },
  {
    "id": "mcp-client-authorization",
    "name": "MCP Client Authorization",
    "architectureDiagram": "images/mcp-client-authorization.gif",
    "categories": [
      "Knowledge & Tools",
      "Governance & Responsible AI"
    ],
    "services": [
      "MCP",
      "Microsoft Entra ID"
    ],
    "shortDescription": "Implement MCP client authorization flow with APIM as OAuth client and authorization server.",
    "detailedDescription": "Playground to experiment with the Model Context Protocol client authorization flow. In this flow, Azure API Management acts both as an OAuth client connecting to Microsoft Entra ID authorization server and as an OAuth authorization server for the MCP client (MCP inspector). Note: Due to the evolving nature of the MCP Authorization proposal, direct production use is not yet recommended.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-client-authorization"
  },
  {
    "id": "mcp-from-api",
    "name": "MCP from REST API",
    "architectureDiagram": "images/model-context-protocol.gif",
    "categories": [
      "Knowledge & Tools"
    ],
    "services": [
      "MCP"
    ],
    "shortDescription": "Transform existing REST APIs to Model Context Protocol with Azure API Management.",
    "detailedDescription": "Playground to transform an existing REST API to the Model Context Protocol with Azure API Management. This lab demonstrates how to expose traditional REST APIs through the MCP standard, enabling AI models to interact with your existing services using the protocol.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-from-api"
  },
  {
    "id": "mcp-prm-oauth",
    "name": "MCP Protected Resources Metadata (PRM)",
    "architectureDiagram": "images/mcp-prm-oauth.gif",
    "categories": [
      "Knowledge & Tools",
      "Governance & Responsible AI"
    ],
    "services": [
      "MCP",
      "Microsoft Entra ID"
    ],
    "shortDescription": "Production-grade MCP implementation with OAuth and Protected Resources Metadata (RFC9729).",
    "detailedDescription": "Playground to experiment with the Model Context Protocol implementing the MCP Authorization proposal and RFC9729 for Protected Resources Metadata. Azure API Management acts as both OAuth client (to Microsoft Entra ID) and OAuth authorization server (for MCP clients like VS Code or Copilot Studio). This is as close to production-grade security for MCP servers as it gets.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-prm-oauth"
  },
  {
    "id": "mcp-registry-apic",
    "name": "MCP Registry with API Center",
    "architectureDiagram": "images/apic-registry.gif",
    "categories": [
      "Knowledge & Tools",
      "Platform Capabilities"
    ],
    "services": [
      "Azure API Center",
      "MCP"
    ],
    "shortDescription": "Centralized MCP server registry using Azure API Center for enterprise governance.",
    "detailedDescription": "To unlock the full potential of Model Context Protocol, enterprises need a centralized registry for server discovery and metadata management. Azure API Center serves as a governed, enterprise-grade repository for managing remote MCP servers. This lab demonstrates creating an API Center service and registering example remote MCP servers with centralized oversight for better version control and access management.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-registry-apic"
  },
  {
    "id": "mcp-registry-apic-github-workflow",
    "name": "MCP Registry with API Center (CI/CD)",
    "architectureDiagram": "images/apic-registry.gif",
    "categories": [
      "Knowledge & Tools",
      "Platform Capabilities"
    ],
    "services": [
      "Azure API Center",
      "MCP",
      "GitHub Actions"
    ],
    "shortDescription": "Automated MCP server registry with Azure API Center using GitOps and GitHub CI/CD pipelines.",
    "detailedDescription": "Demonstrates a fully automated Model Context Protocol server registry integrating with Azure API Center using GitHub CI/CD pipelines. Developers add MCP servers by committing JSON files, GitHub Actions automatically validate and deploy changes, and Azure API Center becomes the centralized registry for organizational MCP server discovery. Complete GitOps solution with infrastructure as code using Bicep.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-registry-apic-github-workflow"
  },
  {
    "id": "message-storing",
    "name": "Message Storing",
    "architectureDiagram": "images/message-storing.gif",
    "categories": [
      "Platform Capabilities"
    ],
    "services": [
      "Azure Cosmos DB",
      "Azure Storage"
    ],
    "shortDescription": "Store conversation history and messages for compliance and analysis.",
    "detailedDescription": "Implement message storage solutions to persist conversation history for compliance, auditing, and analysis purposes. This lab shows how to use Azure API Management policies to store messages in Azure Cosmos DB or Azure Storage, manage retention policies, and retrieve conversation history for continuing sessions or compliance reporting.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/message-storing"
  },
  {
    "id": "model-context-protocol",
    "name": "Model Context Protocol (MCP)",
    "architectureDiagram": "images/model-context-protocol.gif",
    "categories": [
      "Knowledge & Tools",
      "AI Agents"
    ],
    "services": [
      "MCP",
      "Azure OpenAI"
    ],
    "shortDescription": "Implement the Model Context Protocol for standardized AI integrations.",
    "detailedDescription": "Learn to implement the Model Context Protocol (MCP), an open standard for enabling AI models to securely access external data sources and tools. This lab demonstrates how to set up MCP servers, expose them through Azure API Management, and enable AI models to interact with databases, APIs, and other services using this standardized protocol.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/model-context-protocol"
  },
  {
    "id": "model-routing",
    "name": "Model Routing",
    "architectureDiagram": "images/model-routing.gif",
    "categories": [
      "Models Usage",
      "Platform Capabilities"
    ],
    "services": [
      "Azure AI Foundry",
      "Azure OpenAI"
    ],
    "shortDescription": "Intelligently route requests to different AI models based on content and requirements.",
    "detailedDescription": "Implement intelligent model routing strategies to direct requests to the most appropriate AI model based on factors like request content, user tier, cost optimization, or capability requirements. This lab demonstrates how to use Azure API Management policies to analyze incoming requests and route them to different models (GPT-3.5, GPT-4, custom models) for optimal performance and cost efficiency.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/model-routing"
  },
  {
    "id": "openai-agents",
    "name": "OpenAI Agents",
    "architectureDiagram": "images/openai-agents.gif",
    "categories": [
      "AI Agents"
    ],
    "services": [
      "Assistants API",
      "Azure OpenAI"
    ],
    "shortDescription": "Build AI agents using OpenAI's Assistants API through Azure.",
    "detailedDescription": "Explore building AI agents using OpenAI's Assistants API deployed on Azure. This lab demonstrates how to create assistants with custom instructions, enable code interpreter and retrieval capabilities, manage threads and messages, and expose these capabilities through Azure API Management for secure, scalable agent deployments.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/openai-agents"
  },
  {
    "id": "private-connectivity",
    "name": "Private Connectivity",
    "architectureDiagram": "images/private-connectivity.gif",
    "categories": [
      "Governance & Responsible AI",
      "Platform Capabilities"
    ],
    "services": [
      "Private Link",
      "VNet"
    ],
    "shortDescription": "Secure AI services with private network connectivity using Azure Private Link.",
    "detailedDescription": "Implement end-to-end private connectivity for your AI services using Azure Private Link and Virtual Networks. This lab demonstrates how to configure Azure API Management in internal mode, set up private endpoints for Azure OpenAI and other services, implement network security groups, and ensure all traffic remains within your private network for maximum security.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/private-connectivity"
  },
  {
    "id": "realtime-audio",
    "name": "Realtime Audio",
    "architectureDiagram": "images/realtime-audio.gif",
    "categories": [
      "Models Usage"
    ],
    "services": [
      "Audio Agents",
      "Realtime API",
      "Azure OpenAI"
    ],
    "shortDescription": "Enable real-time audio processing and speech-to-speech AI interactions.",
    "detailedDescription": "Build real-time audio applications using Azure OpenAI's audio capabilities through Azure API Management. This lab covers setting up WebSocket connections for streaming audio, implementing speech-to-text and text-to-speech conversions, managing real-time conversations, and handling audio streaming efficiently with low latency.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/realtime-audio"
  },
  {
    "id": "realtime-mcp-agents",
    "name": "Realtime Audio with MCP Agents",
    "architectureDiagram": "images/realtime-mcp-agents.gif",
    "categories": [
      "AI Agents",
      "Models Usage",
      "Knowledge & Tools"
    ],
    "services": [
      "MCP",
      "Audio Agents",
      "Realtime API",
      "Azure OpenAI"
    ],
    "shortDescription": "Realtime audio API with MCP tools integration for weather, Spotify, and ServiceNow.",
    "detailedDescription": "Playground to experiment with Azure OpenAI Realtime API for text and audio with integrations via Model Context Protocol and Azure API Management. Includes MCP servers for weather service, Spotify (playlists, playback control), and ServiceNow incidents. Leverages the credential manager for OAuth 2.0 tokens and client token validation for end-to-end authentication.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/realtime-mcp-agents"
  },
  {
    "id": "secure-responses-api",
    "name": "Secure Responses API",
    "architectureDiagram": "images/built-in-logging.gif",
    "categories": [
      "Governance & Responsible AI",
      "Models Usage"
    ],
    "services": [
      "Azure OpenAI"
    ],
    "shortDescription": "Implement Azure OpenAI Responses API in a secure manner through APIM.",
    "detailedDescription": "Playground to try the Azure OpenAI Responses API in a secure manner. This lab demonstrates how to properly secure and expose the Responses API through Azure API Management, ensuring proper authentication, authorization, and monitoring of API responses.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/secure-responses-api"
  },
  {
    "id": "semantic-caching",
    "name": "Semantic Caching",
    "architectureDiagram": "images/semantic-caching.gif",
    "categories": [
      "Platform Capabilities",
      "Models Usage"
    ],
    "services": [
      "Azure Cache",
      "Azure OpenAI"
    ],
    "shortDescription": "Reduce latency and costs using semantic caching for Azure OpenAI API requests.",
    "detailedDescription": "The azure-openai-semantic-cache-lookup policy conducts a cache lookup of responses on Azure OpenAI Chat Completion API and Completion API requests from a pre-configured external cache. It operates by comparing the vector proximity of the prompt to prior requests and using a specific similarity score threshold. Caching responses helps reduce bandwidth and processing demands on the backend Azure OpenAI API, thus reducing latency perceived by API consumers.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/semantic-caching"
  },
  {
    "id": "session-awareness",
    "name": "Session Awareness",
    "architectureDiagram": "images/session-awareness.gif",
    "categories": [
      "Platform Capabilities",
      "Models Usage"
    ],
    "services": [
      "Azure Cosmos DB",
      "Azure OpenAI"
    ],
    "shortDescription": "Maintain conversation context across multiple requests using session management.",
    "detailedDescription": "Build session-aware AI applications that maintain conversation context across multiple requests. This lab demonstrates how to implement session management using Azure API Management policies, store session state in Azure Cosmos DB, and provide context-aware responses by including conversation history in subsequent requests to Azure OpenAI.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/session-awareness"
  },
  {
    "id": "slm-self-hosting",
    "name": "SLM Self Hosting",
    "architectureDiagram": "images/slm-self-hosting.gif",
    "categories": [
      "Models Usage",
      "Platform Capabilities"
    ],
    "services": [
      "Phi-3",
      "Self-Hosted Gateway"
    ],
    "shortDescription": "Self-host Phi-3 Small Language Model through APIM self-hosted gateway with OpenAI compatibility.",
    "detailedDescription": "Playground to try the self-hosted Phi-3 Small Language Model (SLM) through the APIM self-hosted gateway with OpenAI API compatibility. The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model. The APIM self-hosted gateway enables use cases where the SLM is running on-premises, bringing the gateway to where your APIs are hosted.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/slm-self-hosting"
  },
  {
    "id": "token-metrics-emitting",
    "name": "Token Metrics Emitting",
    "architectureDiagram": "images/token-metrics-emitting.gif",
    "categories": [
      "Platform Capabilities",
      "Governance & Responsible AI"
    ],
    "services": [
      "Azure Monitor",
      "Application Insights"
    ],
    "shortDescription": "Emit and track detailed token usage metrics for cost analysis and optimization.",
    "detailedDescription": "Learn how to emit detailed token usage metrics from Azure API Management to Azure Monitor and Application Insights. This lab demonstrates how to track prompt tokens, completion tokens, and total token usage across different models and users. Build dashboards and alerts to monitor costs and usage patterns in real-time.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/token-metrics-emitting"
  },
  {
    "id": "token-rate-limiting",
    "name": "Token Rate Limiting",
    "architectureDiagram": "images/token-rate-limiting.gif",
    "categories": [
      "Governance & Responsible AI",
      "Platform Capabilities"
    ],
    "services": [
      "Azure OpenAI"
    ],
    "shortDescription": "Control costs and prevent abuse with token-based rate limiting policies.",
    "detailedDescription": "Implement sophisticated token-based rate limiting to control API usage and costs. This lab shows how to set up rate limits based on token consumption rather than just request counts, providing more granular control over Azure OpenAI usage and costs. Learn to configure different rate limits for different user tiers and implement quota management.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/token-rate-limiting"
  },
  {
    "id": "vector-searching",
    "name": "Vector Searching",
    "architectureDiagram": "images/vector-searching.gif",
    "categories": [
      "Knowledge & Tools"
    ],
    "services": [
      "Azure AI Search",
      "Azure OpenAI"
    ],
    "shortDescription": "Implement semantic search using vector embeddings and Azure AI Search.",
    "detailedDescription": "Build a semantic search solution using vector embeddings and Azure AI Search. This lab demonstrates how to generate embeddings using Azure OpenAI, store them in Azure AI Search, and implement efficient vector-based search queries through Azure API Management. Learn to build RAG (Retrieval-Augmented Generation) patterns for enhanced AI responses.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/vector-searching"
  },
  {
    "id": "zero-to-production",
    "name": "Zero to Production",
    "architectureDiagram": "images/backend-pool-load-balancing.gif",
    "categories": [
      "Platform Capabilities",
      "Governance & Responsible AI"
    ],
    "services": [
      "Azure OpenAI"
    ],
    "shortDescription": "Progressive implementation combining load balancing, token metrics, rate limiting, and semantic caching.",
    "detailedDescription": "Playground to create a combination of several policies in an iterative approach. We start with load balancing, then progressively add token emitting, rate limiting, and eventually semantic caching. Each of these sets of policies is derived from other labs in this repo, providing a comprehensive path from basic setup to production-ready configuration.",
    "authors": [
      "dariuszparys"
    ],
    "githubPath": "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/zero-to-production"
  }
]
