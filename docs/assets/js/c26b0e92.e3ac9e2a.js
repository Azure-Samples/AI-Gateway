"use strict";(self.webpackChunkworkshop=self.webpackChunkworkshop||[]).push([[368],{6061:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/AI-Gateway/docs/introduction","docId":"introduction","unlisted":false},{"type":"link","label":"Prerequisites","href":"/AI-Gateway/docs/prerequisites","docId":"prerequisites","unlisted":false},{"type":"category","label":"Azure OpenAI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create Azure resources","href":"/AI-Gateway/docs/azure-openai/create-resources","docId":"azure-openai/create-resources","unlisted":false},{"type":"link","label":"Control cost and performance with token quotas and limits","href":"/AI-Gateway/docs/azure-openai/rate-limit","docId":"azure-openai/rate-limit","unlisted":false},{"type":"link","label":"Keep visibility into AI consumption with model monitoring","href":"/AI-Gateway/docs/azure-openai/track-consumption","docId":"azure-openai/track-consumption","unlisted":false},{"type":"link","label":"Ensure resiliency and optimized resource consumption with load balancer & circuit breaker","href":"/AI-Gateway/docs/azure-openai/dynamic-failover","docId":"azure-openai/dynamic-failover","unlisted":false}],"href":"/AI-Gateway/docs/category/azure-openai"}]},"docs":{"azure-openai/create-resources":{"id":"azure-openai/create-resources","title":"Create Azure resources","description":"Here we will create the needed Azure resources that we will use throughout this section.","sidebar":"tutorialSidebar"},"azure-openai/dynamic-failover":{"id":"azure-openai/dynamic-failover","title":"Ensure resiliency and optimized resource consumption with load balancer & circuit breaker","description":"When the number of users increase to the point that one region or server where the application have trouble responding to requests in a reasonable time. This creates a user experience where the app feels slow. To avoid this poor user experience, load balancing can be used. With \\"load balancing\\" you set up multiple endpoints capable of serving requests and additionaly configure a scheme for how the \\"balancing\\" should happen.","sidebar":"tutorialSidebar"},"azure-openai/rate-limit":{"id":"azure-openai/rate-limit","title":"Control cost and performance with token quotas and limits","description":"Once you bring an LLM to production and exposes it as an API Endpoint, you need to consider how you \\"manage\\" such an API. There are many considerations to be made everything from caching, scaling, error management, rate limiting, monitoring and more.","sidebar":"tutorialSidebar"},"azure-openai/track-consumption":{"id":"azure-openai/track-consumption","title":"Keep visibility into AI consumption with model monitoring","description":"In this lesson we will use the Azure service, Azure API Management and show how by adding one of its policies to an LLM endpoint; you can monitor the usage of tokens.","sidebar":"tutorialSidebar"},"introduction":{"id":"introduction","title":"Introduction","description":"Businesses are building applications that rely heavily on Artificial Intelligence (AI) to deliver advanced functionalities. They may include:","sidebar":"tutorialSidebar"},"prerequisites":{"id":"prerequisites","title":"Prerequisites","description":"To complete this workshop, you will need the following:","sidebar":"tutorialSidebar"}}}}')}}]);