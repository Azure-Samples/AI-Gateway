"use strict";(self.webpackChunkworkshop=self.webpackChunkworkshop||[]).push([[368],{6061:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/AI-Gateway/docs/introduction","docId":"introduction","unlisted":false},{"type":"link","label":"Prerequisites","href":"/AI-Gateway/docs/prerequisites","docId":"prerequisites","unlisted":false},{"type":"category","label":"Azure OpenAI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create Azure resources","href":"/AI-Gateway/docs/azure-openai/create-resources","docId":"azure-openai/create-resources","unlisted":false},{"type":"link","label":"Control cost and performance with token quotas and limits","href":"/AI-Gateway/docs/azure-openai/rate-limit","docId":"azure-openai/rate-limit","unlisted":false},{"type":"link","label":"Keep visibility into AI consumption with model monitoring","href":"/AI-Gateway/docs/azure-openai/track-consumption","docId":"azure-openai/track-consumption","unlisted":false},{"type":"link","label":"Ensure resiliency and optimized resource consumption with load balancer & circuit breaker","href":"/AI-Gateway/docs/azure-openai/dynamic-failover","docId":"azure-openai/dynamic-failover","unlisted":false},{"type":"link","label":"Access control","href":"/AI-Gateway/docs/azure-openai/access-controlling","docId":"azure-openai/access-controlling","unlisted":false},{"type":"link","label":"Function calling","href":"/AI-Gateway/docs/azure-openai/function-calling","docId":"azure-openai/function-calling","unlisted":false},{"type":"link","label":"Semantic caching","href":"/AI-Gateway/docs/azure-openai/semantic-caching","docId":"azure-openai/semantic-caching","unlisted":false}],"href":"/AI-Gateway/docs/category/azure-openai"},{"type":"category","label":"Agents","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"AI Agent Service","href":"/AI-Gateway/docs/agents/ai-agent-service","docId":"agents/ai-agent-service","unlisted":false},{"type":"link","label":"Model Context Protocol (MCP)","href":"/AI-Gateway/docs/agents/mcp","docId":"agents/mcp","unlisted":false},{"type":"link","label":"Open AI Agents","href":"/AI-Gateway/docs/agents/openai-agents","docId":"agents/openai-agents","unlisted":false}],"href":"/AI-Gateway/docs/category/agents"}]},"docs":{"agents/ai-agent-service":{"id":"agents/ai-agent-service","title":"AI Agent Service","description":"coming soon...","sidebar":"tutorialSidebar"},"agents/mcp":{"id":"agents/mcp","title":"Model Context Protocol (MCP)","description":"Coming soon..","sidebar":"tutorialSidebar"},"agents/openai-agents":{"id":"agents/openai-agents","title":"Open AI Agents","description":"Coming soon ...","sidebar":"tutorialSidebar"},"azure-openai/access-controlling":{"id":"azure-openai/access-controlling","title":"Access control","description":"Coming soon ...","sidebar":"tutorialSidebar"},"azure-openai/create-resources":{"id":"azure-openai/create-resources","title":"Create Azure resources","description":"Here we will create the needed Azure resources that we will use throughout this section.","sidebar":"tutorialSidebar"},"azure-openai/dynamic-failover":{"id":"azure-openai/dynamic-failover","title":"Ensure resiliency and optimized resource consumption with load balancer & circuit breaker","description":"When the number of users increase to the point that one region or server where the application have trouble responding to requests in a reasonable time. This creates a user experience where the app feels slow. To avoid this poor user experience, load balancing can be used. With \\"load balancing\\" you set up multiple endpoints capable of serving requests and additionaly configure a scheme for how the \\"balancing\\" should happen.","sidebar":"tutorialSidebar"},"azure-openai/function-calling":{"id":"azure-openai/function-calling","title":"Function calling","description":"coming soon ...","sidebar":"tutorialSidebar"},"azure-openai/rate-limit":{"id":"azure-openai/rate-limit","title":"Control cost and performance with token quotas and limits","description":"Once you bring an LLM to production and exposes it as an API Endpoint, you need to consider how you \\"manage\\" such an API. There are many considerations to be made everything from caching, scaling, error management, rate limiting, monitoring and more.","sidebar":"tutorialSidebar"},"azure-openai/semantic-caching":{"id":"azure-openai/semantic-caching","title":"Semantic caching","description":"coming soon ...","sidebar":"tutorialSidebar"},"azure-openai/track-consumption":{"id":"azure-openai/track-consumption","title":"Keep visibility into AI consumption with model monitoring","description":"In this lesson we will use the Azure service, Azure API Management and show how by adding one of its policies to an LLM endpoint; you can monitor the usage of tokens.","sidebar":"tutorialSidebar"},"introduction":{"id":"introduction","title":"Introduction","description":"Businesses are building applications that rely heavily on Artificial Intelligence (AI) to deliver advanced functionalities. They may include:","sidebar":"tutorialSidebar"},"prerequisites":{"id":"prerequisites","title":"Prerequisites","description":"To complete this workshop, you will need the following:","sidebar":"tutorialSidebar"}}}}')}}]);