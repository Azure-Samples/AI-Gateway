{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è AI Foundry\n",
    "\n",
    "## Test your Azure AI Foundry models, enabled through Azure API Management!\n",
    "\n",
    "Use this Jupyter notebook with Python code snippets to verify proper functionality of your Azure AI Foundry models when accessed through AI Gateway features in Azure API Management (APIM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### ‚öôÔ∏è Initialize client tool for your APIM service\n",
    "\n",
    "üëâ An existing Azure AI Foundry API is expected to be already configured on APIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, requests\n",
    "sys.path.insert(1, '../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "from apimtools import APIMClientTool\n",
    "\n",
    "model_name = \"gpt-4.1\"\n",
    "inference_api_version = \"2025-03-01-preview\"\n",
    "\n",
    "try:\n",
    "    apimClientTool = APIMClientTool(\n",
    "        \"lab-ai-gateway\" ## specify the resource group name where the API Management resource is located, or optionally add another parameter with the apim_resource_name\n",
    "    )\n",
    "    apimClientTool.initialize()\n",
    "    apimClientTool.discover_api('/openai') # replace with /models for inference API\n",
    "\n",
    "    apim_resource_gateway_url = str(apimClientTool.apim_resource_gateway_url)\n",
    "    foundry_project_endpoint = f\"{apim_resource_gateway_url.replace('apim-', 'foundry-').replace('.azure-api.net', '.services.ai.azure.com')}/api/projects/default\"\n",
    "    azure_endpoint = str(apimClientTool.azure_endpoint)\n",
    "    chat_completions_url = f\"{azure_endpoint}/openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "    api_key = apimClientTool.apim_subscriptions[1].get(\"key\") # Ensure that you have created a subscription in APIM\n",
    "\n",
    "    utils.print_ok(f\"Testing tool initialized successfully!\")\n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error initializing APIM Client Tool: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=inference_api_version\n",
    ")\n",
    "response = client.chat.completions.create(model=model_name, messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "])\n",
    "print(\"üí¨ \",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "chat_completions_url = f\"{azure_endpoint}/openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "response = requests.post(chat_completions_url, headers = {'api-key':api_key}, json = messages)\n",
    "utils.print_response_code(response)\n",
    "utils.print_info(f\"headers {response.headers}\")\n",
    "utils.print_info(f\"x-ms-region: {response.headers.get(\"x-ms-region\")}\") # this header is useful to determine the region of the backend that served the request\n",
    "if (response.status_code == 200):\n",
    "    data = json.loads(response.text)\n",
    "    print(\"üí¨ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "else:\n",
    "    utils.print_error(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Send multiple requests to surpass the established token rate limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "api_runs = []\n",
    "for i in range(20):\n",
    "    response = requests.post(chat_completions_url, headers = {'api-key': api_key}, json = messages)\n",
    "    utils.print_response_code(response)\n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        total_tokens = data.get(\"usage\").get(\"total_tokens\")\n",
    "        print(\"üí¨ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "    else:\n",
    "        print(response.text)\n",
    "        total_tokens = 0\n",
    "    api_runs.append((total_tokens, response.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Token Rate limiting results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [15, 7]\n",
    "df = pd.DataFrame(api_runs, columns=['Tokens', 'Status Code'])\n",
    "df['Run'] = range(1, len(df) + 1)\n",
    "colors = ['red' if str(code).startswith('5') else 'yellow' if str(code).startswith('4') else 'lightblue' for code in df['Status Code']]\n",
    "ax = df.plot(kind='bar', x='Run', y='Tokens', color=colors, legend=False)\n",
    "plt.title('Rate Limiting results')\n",
    "plt.xlabel('Runs')\n",
    "plt.ylabel('Tokens')\n",
    "plt.xticks(df['Run'], rotation=0)\n",
    "for i, val in enumerate(df['Status Code']):\n",
    "    ax.text(i, 20, '' if int(val) == 200 else '[429]', ha='center', va='bottom')\n",
    "for i, val in enumerate(df['Tokens']):\n",
    "    ax.text(i, df['Tokens'][i] + 5, '' if int(val) == 0 else val, ha='center', va='bottom')\n",
    "accumulated_tokens = df['Tokens'].cumsum()\n",
    "ax.plot(df['Run']-1, accumulated_tokens, color='green', label='Accumulated Tokens')\n",
    "for i, val in enumerate(accumulated_tokens):\n",
    "    ax.text(i, val + 6, str(int(val)), ha='center', va='bottom', label='Accumulated Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Azure AI Agents'></a>\n",
    "### üß™ Execute an [Azure AI Foundry Agent using MCP Tools](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/model-context-protocol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.agents.models import ListSortOrder, MessageTextContent, McpTool, RequiredMcpToolCall, SubmitToolApprovalAction, ToolApproval\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import time\n",
    "\n",
    "project_client = AIProjectClient(endpoint=foundry_project_endpoint,\n",
    "            credential=DefaultAzureCredential())\n",
    "agents_client = project_client.agents\n",
    "\n",
    "# MCP tool definition\n",
    "mcp_tool = McpTool(\n",
    "    server_label=\"weather\",\n",
    "    server_url=f\"{apim_resource_gateway_url}/weather-mcp/sse\",\n",
    ")\n",
    "\n",
    "prompt = \"What's the weather in San Francisco, Seattle and Lisbon?\"\n",
    "\n",
    "# Agent creation\n",
    "agent = agents_client.create_agent(\n",
    "    model=model_name,\n",
    "    name=\"agent-mcp\",\n",
    "    instructions=\"You are a weather agent.\",\n",
    "    tools=mcp_tool.definitions\n",
    ")\n",
    "\n",
    "print(f\"üéâ Created agent, agent ID: {agent.id}\")\n",
    "print(f\"‚ú® MCP Server: {mcp_tool.server_label} at {mcp_tool.server_url}\")\n",
    "\n",
    "# Thread creation\n",
    "thread = agents_client.threads.create()\n",
    "print(f\"üßµ Created thread, thread ID: {thread.id}\")\n",
    "\n",
    "# Message creation\n",
    "message = agents_client.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=prompt,\n",
    ")\n",
    "print(f\"üí¨ Created message, message ID: {message.id}\")\n",
    "\n",
    "mcp_tool.set_approval_mode(\"never\")          # Disable human approval\n",
    "\n",
    "# Run\n",
    "run = agents_client.runs.create(thread_id=thread.id, agent_id=agent.id, tool_resources=mcp_tool.resources)\n",
    "while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "    time.sleep(2)\n",
    "    run = agents_client.runs.get(thread_id=thread.id, run_id=run.id)\n",
    "    print(f\"‚è≥ Run status: {run.status}\")\n",
    "if run.status == \"failed\":\n",
    "    print(f\"‚ùå Run error: {run.last_error}\")\n",
    "\n",
    "# Get Run steps\n",
    "run_steps = agents_client.run_steps.list(thread_id=thread.id, run_id=run.id)\n",
    "print()\n",
    "\n",
    "for step in run_steps:\n",
    "    print(f\"üîÑ Run step: {step.id}, status: {step.status}, type: {step.type}\")\n",
    "    if step.type == \"tool_calls\":\n",
    "        print(f\"üõ†Ô∏è Tool call details:\")\n",
    "        for tool_call in step.step_details.tool_calls:\n",
    "            print(json.dumps(tool_call.as_dict(), indent=5))\n",
    "\n",
    "# Get the messages in the thread\n",
    "print(\"\\nüìú Messages in the thread:\")\n",
    "messages = agents_client.messages.list(thread_id=thread.id, order=ListSortOrder.ASCENDING)\n",
    "\n",
    "for item in messages:\n",
    "    last_message_content = item.content[-1]\n",
    "    if isinstance(last_message_content, MessageTextContent):\n",
    "        print(f\"üó®Ô∏è {item.role}: {last_message_content.text.value}\")\n",
    "\n",
    "# Clean up resources\n",
    "# agents_client.delete_agent(agent.id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
