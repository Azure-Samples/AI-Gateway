{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è AI Foundry\n",
    "\n",
    "## Test your Azure AI Foundry models, enabled through Azure API Management!\n",
    "\n",
    "Use this Jupyter notebook with Python code snippets to verify proper functionality of your Azure AI Foundry models when accessed through AI Gateway features in Azure API Management (APIM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### ‚öôÔ∏è Initialize client tool for your APIM service\n",
    "\n",
    "üëâ An existing Azure AI Foundry API is expected to be already configured on APIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, requests\n",
    "sys.path.insert(1, '../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "from apimtools import APIMClientTool\n",
    "\n",
    "model_name = \"gpt-4.1\"\n",
    "inference_api_version = \"2025-03-01-preview\"\n",
    "\n",
    "try:\n",
    "    apimClientTool = APIMClientTool(\n",
    "        \"lab-ai-gateway\" ## specify the resource group name where the API Management resource is located, or optionally add another parameter with the apim_resource_name\n",
    "    )\n",
    "    apimClientTool.initialize()\n",
    "    apimClientTool.discover_api('/openai') # replace with /models for inference API\n",
    "\n",
    "    apim_resource_gateway_url = str(apimClientTool.apim_resource_gateway_url)\n",
    "    foundry_project_endpoint = f\"{apim_resource_gateway_url.replace('apim-', 'foundry-').replace('.azure-api.net', '.services.ai.azure.com')}/api/projects/default\"\n",
    "    azure_endpoint = str(apimClientTool.azure_endpoint)\n",
    "    chat_completions_url = f\"{azure_endpoint}/openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "    api_key = apimClientTool.apim_subscriptions[1].get(\"key\") # Ensure that you have created a subscription in APIM\n",
    "\n",
    "    utils.print_ok(f\"Testing tool initialized successfully!\")\n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error initializing APIM Client Tool: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=inference_api_version\n",
    ")\n",
    "response = client.chat.completions.create(model=model_name, messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "])\n",
    "print(\"üí¨ \",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "chat_completions_url = f\"{azure_endpoint}/openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "response = requests.post(chat_completions_url, headers = {'api-key':api_key}, json = messages)\n",
    "utils.print_response_code(response)\n",
    "utils.print_info(f\"headers {response.headers}\")\n",
    "utils.print_info(f\"x-ms-region: {response.headers.get(\"x-ms-region\")}\") # this header is useful to determine the region of the backend that served the request\n",
    "if (response.status_code == 200):\n",
    "    data = json.loads(response.text)\n",
    "    print(\"üí¨ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "else:\n",
    "    utils.print_error(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Send multiple requests within one minute to surpass the established token rate limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "# Run for 1 minute (60 seconds)\n",
    "api_runs = []\n",
    "start_time = time.time()\n",
    "run_count = 0\n",
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "print(f\"üïê Starting API calls for 1 minute...\")\n",
    "print(f\"Start time: {time.strftime('%H:%M:%S', time.localtime(start_time))}\")\n",
    "\n",
    "while (time.time() - start_time) < 60:  # Run for 60 seconds\n",
    "    run_count += 1    \n",
    "    call_start_time = time.time()\n",
    "    response = requests.post(chat_completions_url, headers = {'api-key':api_key}, json = messages)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if (response.status_code == 200):\n",
    "        print(f\"‚ñ∂Ô∏è Run: {run_count} | {elapsed_time:.1f}s | status: {response.status_code} ‚úÖ\")\n",
    "        data = json.loads(response.text)\n",
    "        total_tokens = data.get(\"usage\").get(\"total_tokens\")\n",
    "        print(f\"    consumed tokens: {response.headers.get('consumed-tokens')}, remaining tokens: {response.headers.get('remaining-tokens')}\")\n",
    "    else:\n",
    "        print(f\"‚ñ∂Ô∏è Run: {run_count} | {elapsed_time:.1f}s | status: {response.status_code} ‚õî\")        \n",
    "        print(f\"    error: {response.text}\")\n",
    "        total_tokens = 0\n",
    "    \n",
    "    api_runs.append((call_start_time, total_tokens, response.status_code))\n",
    "    time.sleep(0.1) # Small delay to prevent overwhelming the API\n",
    "\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "print(f\"\\nüèÅ Completed {run_count} API calls in {total_duration:.1f} seconds\")\n",
    "print(f\"Average rate: {run_count / total_duration:.2f} calls/second\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Token Rate limiting results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "if 'api_runs' in locals() and api_runs:\n",
    "    calls = [(t - api_runs[0][0], tokens or 0, status) for t, tokens, status in api_runs]\n",
    "    capacity = 1000\n",
    "    refill = capacity / 60\n",
    "    bucket = capacity\n",
    "    last_time = 0.0\n",
    "    times, usage, status_codes, levels = [], [], [], []\n",
    "\n",
    "    for call_time, tokens, status in calls:\n",
    "        bucket = min(capacity, bucket + (call_time - last_time) * refill)\n",
    "        levels.append(bucket)\n",
    "        times.append(call_time)\n",
    "        usage.append(tokens)\n",
    "        status_codes.append(status)\n",
    "        bucket = max(0, bucket - tokens)\n",
    "        last_time = call_time\n",
    "\n",
    "    colors = ['tab:green' if code == 200 else 'tab:red' if code == 429 else 'tab:orange' for code in status_codes]\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.bar(times, usage, color=colors, width=0.35, alpha=0.7)\n",
    "    ax2.plot(times, levels, color='purple', linewidth=2)\n",
    "    ax2.axhline(capacity, color='purple', linestyle='--', alpha=0.6)\n",
    "\n",
    "    throttled_times = [t for t, code in zip(times, status_codes) if code == 429]\n",
    "    throttled_usage = [u for u, code in zip(usage, status_codes) if code == 429]\n",
    "    if throttled_times:\n",
    "        max_usage = max(usage) if usage else 0\n",
    "        throttled_marker_heights = [u + max_usage * 0.01 for u in throttled_usage]\n",
    "        ax1.scatter(throttled_times, throttled_marker_heights, marker='o', s=20, color='darkred', edgecolors='white', linewidth=0.4, zorder=6)\n",
    "\n",
    "    ax1.set_xlabel('Seconds')\n",
    "    ax1.set_ylabel('Tokens per call')\n",
    "    ax2.set_ylabel('Tokens in bucket')\n",
    "    ax1.set_title('Token bucket behaviour over 60 seconds')\n",
    "\n",
    "    legend_items = [\n",
    "        Patch(facecolor='tab:green', alpha=0.7, label='Success (200)'),\n",
    "        Line2D([0], [0], color='purple', linewidth=2, label='Bucket level'),\n",
    "        Line2D([0], [0], color='purple', linestyle='--', label='Capacity'),\n",
    "        Line2D([0], [0], marker='o', color='darkred', markersize=8, linestyle='None',\n",
    "                markerfacecolor='darkred', markeredgecolor='white', label='Throttled (429)')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_items, loc='upper right', bbox_to_anchor=(0.98, 0.85), framealpha=0.9)\n",
    "\n",
    "    success = sum(code == 200 for code in status_codes)\n",
    "    throttled = sum(code == 429 for code in status_codes)\n",
    "    print(f\"Calls: {len(status_codes)} | Success: {success} | 429s: {throttled}\")\n",
    "else:\n",
    "    print('Run the 60-second API test first to capture api_runs data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Azure AI Agents'></a>\n",
    "### üß™ Execute an [Azure AI Foundry Agent using MCP Tools](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/model-context-protocol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.agents.models import ListSortOrder, MessageTextContent, McpTool, RequiredMcpToolCall, SubmitToolApprovalAction, ToolApproval\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import time\n",
    "\n",
    "weather_mcp_endpoint = \"\"\n",
    "\n",
    "project_client = AIProjectClient(endpoint=foundry_project_endpoint,\n",
    "            credential=DefaultAzureCredential())\n",
    "agents_client = project_client.agents\n",
    "\n",
    "# MCP tool definition\n",
    "mcp_tool = McpTool(\n",
    "    server_label=\"weather\",\n",
    "    server_url=f\"https://{weather_mcp_endpoint}/sse\",\n",
    ")\n",
    "\n",
    "prompt = \"What's the weather in San Francisco, Seattle and Lisbon?\"\n",
    "\n",
    "# Agent creation\n",
    "agent = agents_client.create_agent(\n",
    "    model=model_name,\n",
    "    name=\"agent-mcp\",\n",
    "    instructions=\"You are a weather agent.\",\n",
    "    tools=mcp_tool.definitions\n",
    ")\n",
    "\n",
    "print(f\"üéâ Created agent, agent ID: {agent.id}\")\n",
    "print(f\"‚ú® MCP Server: {mcp_tool.server_label} at {mcp_tool.server_url}\")\n",
    "\n",
    "# Thread creation\n",
    "thread = agents_client.threads.create()\n",
    "print(f\"üßµ Created thread, thread ID: {thread.id}\")\n",
    "\n",
    "# Message creation\n",
    "message = agents_client.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=prompt,\n",
    ")\n",
    "print(f\"üí¨ Created message, message ID: {message.id}\")\n",
    "\n",
    "mcp_tool.set_approval_mode(\"never\")          # Disable human approval\n",
    "\n",
    "# Run\n",
    "run = agents_client.runs.create(thread_id=thread.id, agent_id=agent.id, tool_resources=mcp_tool.resources)\n",
    "while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "    time.sleep(2)\n",
    "    run = agents_client.runs.get(thread_id=thread.id, run_id=run.id)\n",
    "    print(f\"‚è≥ Run status: {run.status}\")\n",
    "if run.status == \"failed\":\n",
    "    print(f\"‚ùå Run error: {run.last_error}\")\n",
    "\n",
    "# Get Run steps\n",
    "run_steps = agents_client.run_steps.list(thread_id=thread.id, run_id=run.id)\n",
    "print()\n",
    "\n",
    "for step in run_steps:\n",
    "    print(f\"üîÑ Run step: {step.id}, status: {step.status}, type: {step.type}\")\n",
    "    if step.type == \"tool_calls\":\n",
    "        print(f\"üõ†Ô∏è Tool call details:\")\n",
    "        for tool_call in step.step_details.tool_calls: ## type: ignore\n",
    "            print(json.dumps(tool_call.as_dict(), indent=5))\n",
    "\n",
    "# Get the messages in the thread\n",
    "print(\"\\nüìú Messages in the thread:\")\n",
    "messages = agents_client.messages.list(thread_id=thread.id, order=ListSortOrder.ASCENDING)\n",
    "\n",
    "for item in messages:\n",
    "    last_message_content = item.content[-1]\n",
    "    if isinstance(last_message_content, MessageTextContent):\n",
    "        print(f\"üó®Ô∏è {item.role}: {last_message_content.text.value}\")\n",
    "\n",
    "# Clean up resources\n",
    "# agents_client.delete_agent(agent.id) # Retain the agent to monitor its execution in AI Foundry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
