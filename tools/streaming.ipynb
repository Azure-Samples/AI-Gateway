{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è AI Foundry\n",
    "\n",
    "## Streaming tests\n",
    "\n",
    "Invoke AI Foundry model API's with stream enabled and returns response in chunks.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "‚ñ∂Ô∏è Click `Run All` to execute all steps sequentially, or execute them `Step by Step`...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### ‚öôÔ∏è Initialize client tool for a given APIM service\n",
    "\n",
    "üëâ An existing Azure OpenAI API is expected to be already configured on APIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, requests\n",
    "sys.path.insert(1, '../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "from apimtools import APIMClientTool\n",
    "\n",
    "model_name = \"gpt-4.1-mini\"\n",
    "inference_api_version = \"2025-03-01-preview\"\n",
    "\n",
    "try:\n",
    "    apimClientTool = APIMClientTool(\n",
    "        \"lab-...\" ## specify the resource group name where the API Management resource is located, or optionally add another parameter with the apim_resource_name\n",
    "    )\n",
    "    apimClientTool.initialize()\n",
    "    apimClientTool.discover_api('/openai') # replace with /models for inference API\n",
    "\n",
    "    azure_endpoint = str(apimClientTool.azure_endpoint)\n",
    "    chat_completions_url = f\"{azure_endpoint}/openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "    api_key = apimClientTool.apim_subscriptions[1].get(\"key\") # Ensure that you have created a subscription in APIM\n",
    "\n",
    "    utils.print_ok(f\"Testing tool initialized successfully!\")\n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error initializing APIM Client Tool: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n",
    "The Python requests library has support for [streaming](https://docs.python-requests.org/en/latest/user/advanced/#streaming-requests). We use the direct http call to inspect the response headers. The policy is injecting a header that identifies if it's a streaming request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests\n",
    "\n",
    "payload={\"messages\":[\n",
    "    {\"role\": \"user\", \"content\": 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "],\n",
    "\"stream\": True}\n",
    "start_time = time.time()  # record the start time of the request\n",
    "response = requests.post(chat_completions_url, headers = {'api-key':api_key, 'Apim-Debug-Authorization': apim_debug_authorization}, json = payload)\n",
    "print(\"status code: \", response.status_code)\n",
    "trace_id = response.headers.get(\"Apim-Trace-Id\")\n",
    "print(\"Apim-Trace-Id: \", trace_id) # this header will be used to get API trace details\n",
    "print(\"headers \", response.headers)\n",
    "print(\"x-ms-region: \", response.headers.get(\"x-ms-region\")) # this header is useful to determine the region of the backend that served the request\n",
    "print(\"x-ms-stream: \", response.headers.get(\"x-ms-stream\")) # this header is useful to determine if the response is streamed\n",
    "if (response.status_code == 200):\n",
    "    for chunk in response.iter_lines():\n",
    "        chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "        print(f\"üß© {chunk_time:.2f} seconds after request: {chunk}\")  # print the delay and text\n",
    "else:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trace1'></a>\n",
    "### üîç Analyze the API trace from direct HTTP call\n",
    "\n",
    "With the following request we will get the json with the complete trace information.\n",
    "\n",
    "üëâ Customize the Notebook layout with scrolling to access the full output trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(apimClientTool.get_trace(trace_id), indent=4)) # this will get the trace details for the request made above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test with streaming using the Azure OpenAI Python SDK\n",
    "With a streaming API call, the response is sent back incrementally in chunks via an [event stream](https://developer.mozilla.org/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). In Python, you can iterate over these events with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "messages=[\n",
    "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "]\n",
    "\n",
    "apim_debug_authorization = apimClientTool.get_debug_credentials(\"PT1H\")\n",
    "\n",
    "start_time = time.time()\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=inference_api_version\n",
    ")\n",
    "response = client.chat.completions.with_raw_response.create(model=model_name, messages=messages, stream=True, extra_headers={'Apim-Debug-Authorization': apim_debug_authorization})\n",
    "\n",
    "print(\"headers \", response.headers)\n",
    "print(\"x-ms-region: \", response.headers.get(\"x-ms-region\")) # this header is useful to determine the region of the backend that served the request\n",
    "print(\"x-ms-stream: \", response.headers.get(\"x-ms-stream\")) # this header is useful to determine if the response is streamed\n",
    "trace_id = response.headers.get(\"Apim-Trace-Id\")\n",
    "print(\"Apim-Trace-Id: \", trace_id) # this header will be used to get API trace details\n",
    "\n",
    "completion = response.parse() \n",
    "\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in completion:\n",
    "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    if chunk.choices:\n",
    "        chunk_message = chunk.choices[0].delta.content  # extract the message\n",
    "        collected_messages.append(chunk_message)  # save the message\n",
    "        print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "# clean None in collected_messages\n",
    "collected_messages = [m for m in collected_messages if m is not None]\n",
    "full_reply_content = ''.join(collected_messages)\n",
    "print(f\"Full conversation received: {full_reply_content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trace2'></a>\n",
    "### üîç Analyze the API trace from the SDK call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(apimClientTool.get_trace(trace_id), indent=4)) # this will get the trace details for the request made above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
