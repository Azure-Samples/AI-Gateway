{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è AI Foundry\n",
    "\n",
    "## Test Apps Authorizations\n",
    "\n",
    "Use this Jupyter notebook containing Python code snippets to validate the functionality of the APIM Applications feature with Azure AI Foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### ‚öôÔ∏è Initialize client tool for your APIM service\n",
    "\n",
    "üëâ An existing Azure OpenAI API is expected to be already configured on APIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, requests\n",
    "sys.path.insert(1, '../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "from apimtools import APIMClientTool\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "inference_api_version = \"2024-10-21\"\n",
    "\n",
    "try:\n",
    "    output = utils.run(\"az account show\", \"Retrieved az account\", \"Failed to get the current az account\")\n",
    "\n",
    "    if output.success and output.json_data:\n",
    "        current_user = output.json_data['user']['name']\n",
    "        tenant_id = output.json_data['tenantId']\n",
    "        subscription_id = output.json_data['id']\n",
    "\n",
    "        utils.print_info(f\"Current user: {current_user}\")\n",
    "        utils.print_info(f\"Tenant ID: {tenant_id}\")\n",
    "        utils.print_info(f\"Subscription ID: {subscription_id}\")\n",
    "\n",
    "    apimClientTool = APIMClientTool(\n",
    "        \"lab-...\" ## specify the resource group name where the API Management resource is located, or optionally add another parameter with the apim_resource_name\n",
    "    )\n",
    "    apimClientTool.initialize()\n",
    "    apimClientTool.discover_api('/openai')\n",
    "\n",
    "    apim_api_endpoint = str(apimClientTool.azure_endpoint)\n",
    "    chat_completions_url = f\"{apim_api_endpoint}/openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "    api_keys = [ apimClientTool.apim_subscriptions[5].get(\"key\"),\n",
    "                apimClientTool.apim_subscriptions[6].get(\"key\"), \n",
    "                apimClientTool.apim_subscriptions[7].get(\"key\"), \n",
    "                apimClientTool.apim_subscriptions[8].get(\"key\") ] \n",
    "    utils.print_ok(f\"Testing tool initialized successfully!\")\n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error initializing APIM Client Tool: {e}\")\n",
    "\n",
    "client_id = \"\" # retrieve the client ID from the APIM Applications overview page\n",
    "client_secret = \"\" # generate a new client secret in the APIM Applications overview page\n",
    "product_app_id = \"\" # retrieve the product app ID from the APIM Product overview page\n",
    "\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "models = [\"gpt-4o-mini\", \"gpt-4.1\", \"gpt-4.1-mini\", \"gpt-4.1-nano\", \"o3-mini\", \"o4-mini\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### ‚öôÔ∏è Generate an OAuth client token to consume the Product API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, json\n",
    "\n",
    "product_app_id = \"ec1687d3-3600-46b0-9e3d-1467eb13aa37\"\n",
    "\n",
    "body = {\n",
    "    \"grant_type\": \"client_credentials\", \"client_id\": client_id, \"client_secret\": client_secret, \"scope\": f\"api://{product_app_id}/.default\"\n",
    "}\n",
    "\n",
    "response = requests.post(f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\", data=body, headers={\"Content-Type\": \"application/x-www-form-urlencoded\"})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    token = response.json().get(\"access_token\")\n",
    "    header, payload, signature = token.split('.')\n",
    "    def pad(b): return b + '=' * (-len(b) % 4)\n",
    "    print(json.dumps(json.loads(base64.urlsafe_b64decode(pad(header)).decode('utf-8')), indent=4))\n",
    "    print(json.dumps(json.loads(base64.urlsafe_b64decode(pad(payload)).decode('utf-8')), indent=4))\n",
    "else:\n",
    "    print(f\"Failed to retrieve token: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_api_endpoint}/{inference_api_path}\",\n",
    "    azure_ad_token=token, # We are using a token and NO API key.\n",
    "    api_version=inference_api_version, \n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(model=model_name, messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "])\n",
    "\n",
    "print(\"üí¨ \",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Send multiple random prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, random, time\n",
    "\n",
    "with open(\"sample-prompts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sample_prompts = json.load(f)\n",
    "\n",
    "api_runs = []\n",
    "for i in range(10):\n",
    "    # prompt = random.choice(sample_prompts)\n",
    "    prompt = sample_prompts[0]\n",
    "\n",
    "    messages={\"messages\":[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt.get(\"question\")}\n",
    "    ]}\n",
    "\n",
    "    print(f\"üí¨ \", prompt.get(\"question\"))\n",
    "    start_time = time.time()\n",
    "    response = requests.post(chat_completions_url, \n",
    "                             headers={'api-key': random.choice(api_keys)}, json = messages)\n",
    "    response_time = time.time() - start_time\n",
    "    utils.print_response_code(response)\n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        total_tokens = data.get(\"usage\").get(\"total_tokens\")\n",
    "        print(f\"‚åö {response_time:.2f} seconds. üó®Ô∏è \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "    else:\n",
    "        print(response.text)\n",
    "        total_tokens = 0\n",
    "    api_runs.append((total_tokens, response.status_code, response_time))\n",
    "    print(\"------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Token Rate limiting results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [15, 7]\n",
    "df = pd.DataFrame(api_runs, columns=['Tokens', 'Status Code', 'Response Time'])\n",
    "df['Run'] = range(1, len(df) + 1)\n",
    "colors = ['red' if str(code).startswith('5') else 'yellow' if str(code).startswith('4') else 'lightblue' for code in df['Status Code']]\n",
    "ax = df.plot(kind='bar', x='Run', y='Tokens', color=colors, legend=False)\n",
    "plt.title('Rate Limiting results')\n",
    "plt.xlabel('Runs')\n",
    "plt.ylabel('Tokens')\n",
    "plt.xticks(df['Run'], rotation=0)\n",
    "for i, val in enumerate(df['Status Code']):\n",
    "    ax.text(i, 20, '' if int(val) == 200 else '[429]', ha='center', va='bottom')\n",
    "for i, val in enumerate(df['Tokens']):\n",
    "    ax.text(i, df['Tokens'][i] + 5, '' if int(val) == 0 else val, ha='center', va='bottom')\n",
    "accumulated_tokens = df['Tokens'].cumsum()\n",
    "ax.plot(df['Run']-1, accumulated_tokens, color='green', label='Accumulated Tokens')\n",
    "for i, val in enumerate(accumulated_tokens):\n",
    "    ax.text(i, val + 6, str(int(val)), ha='center', va='bottom', label='Accumulated Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Semantic Caching performance\n",
    "\n",
    "The first request should take a longer time as it makes it all the way to the Azure OpenAI backend. The subsequent requests should be much quicker as they draw from the semantic cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "df = pd.DataFrame(api_runs, columns=['Tokens', 'Status Code', 'Response Time'])\n",
    "df['Run'] = range(1, len(df) + 1)\n",
    "df.plot(kind='bar', x='Run', y='Response Time', legend=False)\n",
    "plt.title('Semantic Caching Performance')\n",
    "plt.xlabel('Runs')\n",
    "plt.ylabel('Response Time (s)')\n",
    "plt.xticks(rotation=0)  # Set x-axis ticks to be the run numbers\n",
    "\n",
    "average = df['Response Time'].mean()\n",
    "plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "response = requests.post(chat_completions_url, \n",
    "                         headers={'Authorization': 'Bearer ' + token}, json=messages)\n",
    "\n",
    "utils.print_response_code(response)\n",
    "utils.print_info(f\"Headers:\")\n",
    "pprint(dict(response.headers))\n",
    "if (response.status_code == 200):\n",
    "    data = json.loads(response.text)\n",
    "    print(\"üí¨ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "else:\n",
    "    utils.print_error(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
